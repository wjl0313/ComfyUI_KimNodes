from PIL import Image, ImageOps
import numpy as np
from ultralytics import YOLO
import torch
import logging
import os
import os.path

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å®šä¹‰æ¨¡å‹ç›®å½•ä¸ºç›¸å¯¹è·¯å¾„ï¼šæ ¹ç›®å½•ä¸‹çš„ /models/yolo
base_dir = os.getcwd()  # è·å–å½“å‰å·¥ä½œç›®å½•
model_path = os.path.join(base_dir, "models", "yolo")


# å®šä¹‰ get_files å‡½æ•°
def get_files(directory, extensions):
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹æŒ‡å®šæ‰©å±•åçš„æ–‡ä»¶ã€‚

    :param directory: è¦æœç´¢çš„ç›®å½•è·¯å¾„
    :param extensions: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œä¾‹å¦‚ [".pt"]
    :return: å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶åï¼Œå€¼ä¸ºå®Œæ•´è·¯å¾„
    """
    files = {}
    if not os.path.exists(directory):
        logging.warning(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
        return files

    for filename in os.listdir(directory):
        if any(filename.endswith(ext) for ext in extensions):
            files[filename] = os.path.join(directory, filename)
    return files

class FaceDetectionNode:
    """
    Face Detection Node for processing images and detecting faces.
    """
    # æ¨¡å‹ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
    model_cache = {}

    @classmethod
    def INPUT_TYPES(cls):
        model_ext = [".pt"]
        FILES_DICT = get_files(model_path, model_ext)
        FILE_LIST = list(FILES_DICT.keys())
        # logger.info(f"Model path: {model_path}")
        # logger.info(f"Available models: {FILE_LIST}")
        return {
            "required": {
                "image": ("IMAGE", ),
                "yolo_model": (FILE_LIST,),
                "confidence": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.01,
                    "display": "slider",
                }),
                "expand_percent": ("FLOAT", {
                    "default": 0.0,
                    "min": 0.0,
                    "max": 50.0,
                    "step": 1.0,
                    "display": "slider",
                })
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "Face_yolo"
    CATEGORY = "ğŸŠ Kim-Nodes"

    def __init__(self):
        pass

    def Face_yolo(self, image, confidence, expand_percent, yolo_model):
        logger.info(f"Executing face detection with confidence={confidence}, expand_percent={expand_percent}")

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if yolo_model in self.model_cache:
            model = self.model_cache[yolo_model]
            logger.info(f"Using cached model: {yolo_model}")
        else:
            # åŠ è½½æ¨¡å‹
            full_model_path = os.path.join(model_path, yolo_model)
            logger.info(f"Loading YOLO model from {full_model_path}")
            try:
                model = YOLO(full_model_path)
                self.model_cache[yolo_model] = model  # ç¼“å­˜æ¨¡å‹
            except Exception as e:
                logger.error(f"Failed to load YOLO model: {e}")
                raise

        # ç¡®å®šè®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {device}")

        # æ£€æŸ¥å¹¶è°ƒæ•´è¾“å…¥å›¾åƒå½¢çŠ¶
        logger.debug(f"è¾“å…¥çš„ image ç±»å‹: {type(image)}")
        if isinstance(image, torch.Tensor):
            logger.debug(f"è¾“å…¥çš„ image å¼ é‡å½¢çŠ¶: {image.shape}")
            # è°ƒæ•´è¾“å…¥å›¾åƒå½¢çŠ¶ä¸º (batch_size, channels, height, width)
            if len(image.shape) == 4 and image.shape[-1] in [1, 3, 4]:
                # å°†æœ€åä¸€ç»´çš„é€šé“ç§»åŠ¨åˆ°ç¬¬äºŒç»´
                image = image.permute(0, 3, 1, 2)  # å½¢çŠ¶ä» (batch_size, height, width, channels) è½¬ä¸º (batch_size, channels, height, width)
            elif len(image.shape) != 4 or image.shape[1] not in [1, 3, 4]:
                raise ValueError(f"è¾“å…¥çš„ image å½¢çŠ¶ä¸æ­£ç¡®ï¼Œåº”ä¸º (batch_size, channels, height, width)ï¼Œä½†å¾—åˆ° {image.shape}")

            # å°† PyTorch å¼ é‡è½¬æ¢ä¸º NumPy æ•°ç»„
            image_np = image.cpu().numpy()

            # å»é™¤æ‰¹æ¬¡ç»´åº¦
            image_np = image_np[0]  # å½¢çŠ¶: (channels, height, width)

            # è°ƒæ•´å½¢çŠ¶ä¸º (height, width, channels)
            image_np = np.transpose(image_np, (1, 2, 0))
            logger.debug(f"å¤„ç†åçš„å›¾åƒå½¢çŠ¶: {image_np.shape}")

        else:
            raise TypeError("è¾“å…¥çš„ image ä¸æ˜¯ torch.Tensor ç±»å‹")


        # ç¡®ä¿è¾“å…¥æ˜¯æ ‡å‡†æ ¼å¼ (height, width, channels)
        channels = image_np.shape[2]
        if channels == 1:
            # ç°åº¦å›¾åƒï¼Œè½¬æ¢ä¸º RGB
            logger.debug("å°†ç°åº¦å›¾åƒè½¬æ¢ä¸º RGB æ ¼å¼ã€‚")
            image_np = np.repeat(image_np, 3, axis=2)
            channels = 3
        elif channels == 4:
            # RGBA å›¾åƒï¼Œè½¬æ¢ä¸º RGB
            logger.debug("å°† RGBA å›¾åƒè½¬æ¢ä¸º RGB æ ¼å¼ã€‚")
            image_np = image_np[:, :, :3]
            channels = 3

        if channels != 3:
            raise ValueError(f"æœŸæœ›å›¾åƒæœ‰ 3 ä¸ªé€šé“ï¼Œä½†å¾—åˆ° {channels} ä¸ªé€šé“ã€‚")

        logger.debug(f"æœ€ç»ˆå›¾åƒå°ºå¯¸ - é«˜åº¦: {image_np.shape[0]}, å®½åº¦: {image_np.shape[1]}, é€šé“æ•°: {channels}")

        # å°†åƒç´ å€¼ä» [0,1] è½¬æ¢ä¸º [0,255]
        if image_np.dtype != np.uint8:
            image_np = (image_np * 255).astype(np.uint8)

        # å°† NumPy æ•°ç»„è½¬æ¢ä¸º PIL å›¾åƒ
        image_pil = Image.fromarray(image_np)

        # ä½¿ç”¨ YOLO æ¨¡å‹æ£€æµ‹è„¸éƒ¨
        try:
            results = model.predict(source=image_np, conf=confidence, device=device)
        except AttributeError as e:
            logger.error(f"Error during prediction: {e}")
            raise

        bboxes = []
        for result in results:
            for box in result.boxes:
                xmin, ymin, xmax, ymax = box.xyxy[0].cpu().numpy()
                conf = box.conf.cpu().numpy()
                cls = box.cls.cpu().numpy()

                if cls == 0 and conf >= confidence:  # ç¡®ä¿æ£€æµ‹åˆ°è„¸éƒ¨
                    bboxes.append({
                        "xmin": xmin / image_np.shape[1],
                        "ymin": ymin / image_np.shape[0],
                        "width": (xmax - xmin) / image_np.shape[1],
                        "height": (ymax - ymin) / image_np.shape[0],
                    })

        if not bboxes:
            logger.warning("No faces detected.")
            # å¤„ç†è¾“å‡ºå¹¶è¿”å›åŸå§‹å›¾åƒ
            result_tensor = self.process_output(image_pil)
            return (result_tensor,)  # è¿”å›å¤„ç†åçš„åŸå›¾

        cropped_faces = []
        for bbox in bboxes:
            cropped_face = self.crop_face(image_pil, bbox, expand_percent)
            cropped_faces.append(cropped_face)

        # è¿™é‡Œåªå¤„ç†ç¬¬ä¸€ä¸ªè£å‰ªçš„è„¸éƒ¨å›¾åƒ
        result_tensor = self.process_output(cropped_faces[0])
        return (result_tensor,)


    def crop_face(self, image, bbox, expand_percent):
        """
        Crop a face from the image based on bounding box.
        """
        width, height = image.size
        left = int(bbox["xmin"] * width)
        top = int(bbox["ymin"] * height)
        right = left + int(bbox["width"] * width)
        bottom = top + int(bbox["height"] * height)

        # æ‰©å±•è¾¹ç•Œæ¡†
        expand_x = int((right - left) * expand_percent / 100)
        expand_y = int((bottom - top) * expand_percent / 100)
        left = max(0, left - expand_x)
        top = max(0, top - expand_y)
        right = min(width, right + expand_x)
        bottom = min(height, bottom + expand_y)

        # è£å‰ªå¹¶è¿”å›
        cropped_image = image.crop((left, top, right, bottom))

        # ç¡®ä¿è¿”å›çš„æ˜¯ RGB å›¾åƒ
        if cropped_image.mode != "RGB":
            cropped_image = cropped_image.convert("RGB")

        return cropped_image


    def process_output(self, scene_image_pil):
        """
        å°†ç»“æœè½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼è¿”å›ã€‚
        """
        # å°† PIL å›¾åƒè½¬æ¢ä¸º NumPy æ•°ç»„å¹¶å½’ä¸€åŒ–
        result_image = np.array(scene_image_pil).astype(np.float32) / 255.0
        print(f"[DEBUG] è¾“å‡ºç»“æœ (result_image) çš„ç»´åº¦: {result_image.shape}")

        # å¦‚æœç»“æœæ˜¯ RGBA (H, W, 4)ï¼Œéœ€è¦è½¬æ¢å› RGB (H, W, 3)
        if result_image.shape[-1] == 4:
            # ä½¿ç”¨ alpha é€šé“è¿›è¡Œæ··åˆ
            alpha = result_image[..., 3:4]
            rgb = result_image[..., :3]
            result_image = rgb
            print(f"[DEBUG] è½¬æ¢åçš„ RGB å›¾åƒç»´åº¦: {result_image.shape}")

        # å¦‚æœç»“æœæ˜¯ç°åº¦å›¾åƒ (H, W, 1)ï¼Œéœ€è¦è½¬æ¢ä¸º RGB
        if result_image.shape[-1] == 1:
            result_image = np.repeat(result_image, 3, axis=-1)
            print(f"[DEBUG] ç°åº¦å›¾åƒè½¬æ¢ä¸º RGB åçš„ç»´åº¦: {result_image.shape}")

        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        result_image = np.expand_dims(result_image, axis=0)
        print(f"[DEBUG] æ·»åŠ æ‰¹æ¬¡ç»´åº¦åçš„ result_image ç»´åº¦: {result_image.shape}")

        # è½¬æ¢ä¸ºå¼ é‡
        result_tensor = torch.from_numpy(result_image)
        print(f"[DEBUG] è¾“å‡º result_tensor çš„ç»´åº¦: {result_tensor.shape}")

        return result_tensor


