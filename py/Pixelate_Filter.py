import numpy as np
import cv2
import torch
import torchvision.transforms as T

class Pixelate_Filter:
    """
    ä¸€ä¸ªå›¾åƒå¤„ç†èŠ‚ç‚¹ï¼Œå°†å›¾åƒè½¬æ¢ä¸ºåƒç´ ç”»æ•ˆæœã€‚
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE", {
                    "description": "è¾“å…¥å›¾åƒã€‚"
                }),
                "mode": (["lanczos4", "contrast"], {
                    "default": "lanczos4",
                    "description": "åƒç´ åŒ–æ¨¡å¼ã€‚lanczos4ï¼šé«˜è´¨é‡ç¼©æ”¾ï¼›contrastï¼šå¯¹æ¯”åº¦ä¿ç•™ã€‚"
                }),
                "size": ("INT", {
                    "default": 128,
                    "min": 32,
                    "max": 256,
                    "step": 4,
                    "description": "ç›®æ ‡å°ºå¯¸ï¼ˆè¾ƒé•¿è¾¹å°†è¢«ç¼©æ”¾åˆ°è¿™ä¸ªå°ºå¯¸ï¼‰ã€‚"
                }),
                "block_size": ("INT", {
                    "default": 16,
                    "min": 4,
                    "max": 32,
                    "step": 2,
                    "description": "åƒç´ å—å¤§å°ï¼ˆä»…åœ¨contrastæ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼‰ã€‚"
                }),
                "edge_thickness": ("INT", {
                    "default": 4,
                    "min": 1,
                    "max": 16,
                    "step": 1,
                    "description": "åƒç´ è¾¹ç¼˜åšåº¦ï¼ˆä»…åœ¨contrastæ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼‰ã€‚"
                }),
                "colors": ("INT", {
                    "default": 128,
                    "min": 2,
                    "max": 256,
                    "step": 1,
                    "description": "é¢œè‰²æ•°é‡ï¼Œå‡å°‘å¯è·å¾—æ›´å¤å¤çš„æ•ˆæœã€‚"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "execute"
    CATEGORY = "ğŸ’ Kim-Nodes/ğŸ¨Filter | æ»¤é•œ"

    def execute(self, image, mode, size, block_size, edge_thickness, colors):
        # å°†è¾“å…¥è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
        image = image.clone().mul(255).clamp(0, 255).byte().cpu().numpy()
        output = []

        # å¤„ç†æ¯å¼ å›¾ç‰‡
        if len(image.shape) == 4:  # æ‰¹å¤„ç†
            for img in image:
                if mode == "contrast":
                    # ä½¿ç”¨ pixeloe çš„ contrast æ¨¡å¼
                    processed = self.process_contrast(img, size, block_size, edge_thickness, colors)
                else:
                    # ä½¿ç”¨è‡ªå®šä¹‰çš„ lanczos4 æ¨¡å¼
                    processed = self.process_lanczos4(img, size, colors)
                output.append(processed)
        else:  # å•å¼ å›¾ç‰‡
            if mode == "contrast":
                processed = self.process_contrast(image, size, block_size, edge_thickness, colors)
            else:
                processed = self.process_lanczos4(image, size, colors)
            output.append(processed)

        # å †å å¹¶è°ƒæ•´ç»´åº¦é¡ºåº
        output = torch.stack(output, dim=0).permute([0, 2, 3, 1])
        return (output,)

    def process_contrast(self, image, size, block_size, edge_thickness, colors):
        """ä½¿ç”¨ pixeloe çš„ contrast æ¨¡å¼å¤„ç†å›¾åƒ"""
        
        try:
            # å°è¯• torch ç‰ˆæœ¬
            from pixeloe.torch.pixelize import pixelize
            
            # å°†å›¾åƒè½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼ [B,C,H,W] range [0..1]
            if len(image.shape) == 3:  # HWC -> CHW
                img_tensor = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0
                img_tensor = img_tensor.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
            else:
                img_tensor = torch.from_numpy(image).float() / 255.0
            
            # ä½¿ç”¨ torch ç‰ˆæœ¬çš„å‚æ•°
            result = pixelize(
                img_tensor,
                pixel_size=block_size,
                thickness=edge_thickness,
                mode="contrast",
                do_color_match=True,
                do_quant=(colors < 256),
                num_colors=colors if colors < 256 else 32,
                no_post_upscale=False
            )
            # è½¬æ¢å› numpy æ ¼å¼
            img = (result.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            
        except (ImportError, TypeError):
            try:
                # å¦‚æœ torch ç‰ˆæœ¬å¤±è´¥ï¼Œå°è¯• legacy ç‰ˆæœ¬
                from pixeloe.legacy.pixelize import pixelize
                img = pixelize(
                    image,
                    mode="contrast", 
                    contrast=1.0,
                    saturation=1.0,
                    colors=colors if colors < 256 else None,
                    color_quant_method='kmeans',
                    no_upscale=False
                )
                
                # legacy ç‰ˆæœ¬å¯èƒ½éœ€è¦é¢å¤–çš„é¢œè‰²é‡åŒ–
                if colors < 256 and isinstance(img, np.ndarray):
                    img = self.quantize_colors(img, colors)
                    
            except ImportError as e:
                raise ImportError(
                    "æ— æ³•å¯¼å…¥ pixeloe æ¨¡å—ã€‚è¯·ç¡®ä¿å·²å®‰è£… pixeloe åŒ…ã€‚\n"
                    "æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                    "pip install pixeloe\n"
                    f"é”™è¯¯è¯¦æƒ…: {str(e)}"
                )
        
        # è½¬æ¢ä¸º tensor
        return T.ToTensor()(img)

    def process_lanczos4(self, image, size, colors):
        """ä½¿ç”¨ lanczos4 æ¨¡å¼å¤„ç†å›¾åƒ"""
        # è·å–åŸå§‹å°ºå¯¸
        h, w = image.shape[:2]
        
        # è®¡ç®—ç›®æ ‡å°ºå¯¸ï¼Œä¿æŒå®½é«˜æ¯”
        if h > w:
            new_h = size
            new_w = int(w * size / h)
        else:
            new_w = size
            new_h = int(h * size / w)
        
        # ä½¿ç”¨ LANCZOS4 ç¼©å°å›¾åƒ
        small_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)
        
        # åº”ç”¨é¢œè‰²é‡åŒ–ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if colors < 256:
            small_image = self.quantize_colors(small_image, colors)
        
        # æ”¾å¤§å›åŸå§‹å°ºå¯¸ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        small_image = cv2.resize(small_image, (w, h), interpolation=cv2.INTER_NEAREST)
        
        # è½¬æ¢ä¸º tensor
        return T.ToTensor()(small_image)

    def quantize_colors(self, image, colors):
        """å‡å°‘é¢œè‰²æ•°é‡ä»¥è·å¾—æ›´å¤å¤çš„åƒç´ ç”»æ•ˆæœ"""
        # è®¡ç®—æ¯ä¸ªé€šé“çš„é‡åŒ–å› å­
        factor = 256 / colors
        
        # é‡åŒ–é¢œè‰²
        quantized = np.round(image / factor) * factor
        
        # ç¡®ä¿å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
        quantized = np.clip(quantized, 0, 255).astype(np.uint8)
        
        return quantized 