import os
import json
import logging
from typing import Dict, Any, Optional
import folder_paths

try:
    from safetensors import safe_open
    SAFETENSORS_AVAILABLE = True
except ImportError:
    SAFETENSORS_AVAILABLE = False

class LoRA_Metadata_Reader:
    """
    LoRAå…ƒæ•°æ®è¯»å–èŠ‚ç‚¹
    
    ç”¨äºè¯»å–LoRAæ¨¡å‹æ–‡ä»¶çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ŒåŒ…æ‹¬è®­ç»ƒå‚æ•°ã€æ ‡ç­¾é¢‘ç‡ã€æ•°æ®é›†ä¿¡æ¯ç­‰ã€‚
    æ”¯æŒ.safetensorsæ ¼å¼çš„LoRAæ¨¡å‹æ–‡ä»¶ã€‚
    """

    def __init__(self):
        self.lora_dir = folder_paths.get_folder_paths("loras")[0] if folder_paths.get_folder_paths("loras") else ""

    @classmethod
    def INPUT_TYPES(cls):
        # è·å–LoRAæ¨¡å‹æ–‡ä»¶åˆ—è¡¨
        lora_files = []
        if folder_paths.get_folder_paths("loras"):
            lora_dir = folder_paths.get_folder_paths("loras")[0]
            if os.path.exists(lora_dir):
                for file in os.listdir(lora_dir):
                    if file.endswith('.safetensors'):
                        lora_files.append(file)
        
        if not lora_files:
            lora_files = ["None"]

        return {
            "required": {
                "lora_file": (lora_files, {
                    "default": lora_files[0] if lora_files else "None"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("å…ƒæ•°æ®",)
    FUNCTION = "read_lora_metadata"
    CATEGORY = "ğŸ’ Kim-Nodes/ğŸ”¢Metadata | å…ƒæ•°æ®å¤„ç†"

    def read_safetensors_metadata(self, file_path: str) -> Optional[Dict[str, Any]]:
        """è¯»å–safetensorsæ–‡ä»¶çš„å…ƒæ•°æ®"""
        if not SAFETENSORS_AVAILABLE:
            logging.error("safetensorsåº“æœªå®‰è£…ï¼Œæ— æ³•è¯»å–å…ƒæ•°æ®")
            return None
        
        try:
            # ç›´æ¥è¯»å–safetensorsæ–‡ä»¶çš„headeræ¥è·å–å…ƒæ•°æ®
            import struct
            with open(file_path, 'rb') as f:
                # è¯»å–headeré•¿åº¦ (å‰8å­—èŠ‚)
                header_size_bytes = f.read(8)
                if len(header_size_bytes) < 8:
                    return None
                header_size = struct.unpack('<Q', header_size_bytes)[0]
                
                # è¯»å–headerå†…å®¹
                header_bytes = f.read(header_size)
                if len(header_bytes) < header_size:
                    return None
                
                header = json.loads(header_bytes.decode('utf-8'))
                metadata = header.get('__metadata__', {})
                return metadata
        except Exception as e:
            logging.error(f"è¯»å–safetensorså…ƒæ•°æ®å¤±è´¥: {e}")
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨safe_open
            try:
                with safe_open(file_path, framework="pt", device="cpu") as f:
                    metadata = f.metadata()
                    return metadata
            except Exception as e2:
                logging.error(f"å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                return None



    def read_lora_metadata(self, lora_file: str):
        """
        è¯»å–LoRAæ¨¡å‹çš„å…ƒæ•°æ®ä¿¡æ¯
        """
        if lora_file == "None":
            return ("æœªé€‰æ‹©LoRAæ–‡ä»¶",)

        # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
        if self.lora_dir:
            file_path = os.path.join(self.lora_dir, lora_file)
        else:
            file_path = lora_file

        if not os.path.exists(file_path):
            logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return (f"é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨\nè·¯å¾„ï¼š{file_path}",)

                # è¯»å–å…ƒæ•°æ®
        metadata = self.read_safetensors_metadata(file_path)
        
        if metadata is None:
            return (f"é”™è¯¯ï¼šæ— æ³•è¯»å–å…ƒæ•°æ®\næ–‡ä»¶è·¯å¾„ï¼š{file_path}",)
        
        # å›ºå®šå‚æ•°è®¾ç½®ï¼ˆå§‹ç»ˆæ˜¾ç¤ºå®Œæ•´ä¿¡æ¯ï¼‰
        include_training_params = True
        include_tag_frequency = True
        include_dataset_info = True
        max_tags = 100  # æ˜¾ç¤ºå‰100ä¸ªæ ‡ç­¾
        
        # æå–åŸºæœ¬ä¿¡æ¯
        model_name = os.path.splitext(lora_file)[0]
        display_name = metadata.get("ssmd_display_name", model_name) if metadata else model_name
        author = metadata.get("ssmd_author", "æœªçŸ¥") if metadata else "æœªçŸ¥"
        description = metadata.get("ssmd_description", "æ— æè¿°") if metadata else "æ— æè¿°"
        keywords = metadata.get("ssmd_keywords", "æ— å…³é”®è¯") if metadata else "æ— å…³é”®è¯"
        tags = metadata.get("ssmd_tags", "æ— æ ‡ç­¾") if metadata else "æ— æ ‡ç­¾"

        # æ„å»ºå®Œæ•´å…ƒæ•°æ®å­—å…¸ï¼Œåªæ·»åŠ æœ‰æ•ˆæ•°æ®
        full_metadata = {}
        
        # åŸºæœ¬ä¿¡æ¯éƒ¨åˆ† - åªæ·»åŠ éç©ºå€¼
        basic_info = {}
        if model_name and model_name != "None":
            basic_info["æ¨¡å‹åç§°"] = model_name
        if display_name and display_name != model_name and display_name != "æœªçŸ¥":
            basic_info["æ˜¾ç¤ºåç§°"] = display_name
        if author and author != "æœªçŸ¥":
            basic_info["ä½œè€…"] = author
        if description and description != "æ— æè¿°":
            basic_info["æè¿°"] = description
        if keywords and keywords != "æ— å…³é”®è¯":
            basic_info["å…³é”®è¯"] = keywords
        if tags and tags != "æ— æ ‡ç­¾":
            basic_info["æ ‡ç­¾"] = tags
            
        # è¯„åˆ†
        rating = metadata.get("ssmd_rating", "0") if metadata else "0"
        if rating and rating != "0":
            basic_info["è¯„åˆ†"] = rating
            
        # æ¥æºä¿¡æ¯
        source = metadata.get("ssmd_source", "") if metadata else ""
        if source and source != "æœªçŸ¥":
            basic_info["æ¥æº"] = source
            
        # ç‰ˆæœ¬ä¿¡æ¯
        version = metadata.get("ssmd_version", "") if metadata else ""
        if version and version != "æœªçŸ¥":
            basic_info["ç‰ˆæœ¬"] = version
            
        # å“ˆå¸Œå€¼
        model_hash = metadata.get("sshs_model_hash", "") if metadata else ""
        legacy_hash = metadata.get("sshs_legacy_hash", "") if metadata else ""
        if model_hash and model_hash != "æœªçŸ¥":
            basic_info["æ¨¡å‹å“ˆå¸Œ"] = model_hash
        if legacy_hash and legacy_hash != "æœªçŸ¥":
            basic_info["ä¼ ç»Ÿå“ˆå¸Œ"] = legacy_hash
            
        # åªæœ‰å½“åŸºæœ¬ä¿¡æ¯éç©ºæ—¶æ‰æ·»åŠ 
        if basic_info:
            full_metadata["åŸºæœ¬ä¿¡æ¯"] = basic_info

        # æ·»åŠ è®­ç»ƒå‚æ•°ï¼ˆss_å‰ç¼€çš„å‚æ•°ï¼‰
        if include_training_params and metadata:
            training_params = {}
            
            # ä¿ç•™åŸå§‹å‚æ•°åï¼Œä¸ç¿»è¯‘
            
            # éœ€è¦æ’é™¤çš„å†…éƒ¨å‚æ•°
            excluded_params = {
                "ss_bucket_info",  # å†…éƒ¨åˆ†æ¡¶ä¿¡æ¯
                "ss_sd_scripts_commit_hash",  # è„šæœ¬ç‰ˆæœ¬å“ˆå¸Œ
                "ss_model_prediction_type",  # æ¨¡å‹é¢„æµ‹ç±»å‹
                "ss_loss_type",  # æŸå¤±ç±»å‹
                "ss_huber_scale", "ss_huber_c", "ss_huber_schedule",  # HuberæŸå¤±å‚æ•°
                "ss_logit_mean", "ss_logit_std",  # Logitå‚æ•°
                "ss_mode_scale",  # æ¨¡å¼ç¼©æ”¾
                "ss_discrete_flow_shift",  # ç¦»æ•£æµåç§»
                "ss_weighting_scheme",  # æƒé‡æ–¹æ¡ˆ
                "ss_timestep_sampling",  # æ—¶é—´æ­¥é‡‡æ ·
                "ss_sigmoid_scale",  # Sigmoidç¼©æ”¾
                "ss_debiased_estimation",  # å»åä¼°è®¡
                "ss_ip_noise_gamma_random_strength",  # IPå™ªå£°éšæœºå¼ºåº¦
                "ss_noise_offset_random_strength",  # å™ªå£°åç§»éšæœºå¼ºåº¦
                "ss_validation_split", "ss_validation_seed",  # éªŒè¯å‚æ•°
                "ss_validate_every_n_steps", "ss_validate_every_n_epochs",  # éªŒè¯é¢‘ç‡
                "ss_max_validation_steps",  # æœ€å¤§éªŒè¯æ­¥æ•°
                "ss_num_validation_images",  # éªŒè¯å›¾åƒæ•°
                "ss_num_train_images",  # è®­ç»ƒå›¾åƒæ•°
                "ss_num_batches_per_epoch",  # æ¯è½®æ‰¹æ¬¡æ•°
                "ss_guidance_scale",  # å¼•å¯¼ç¼©æ”¾
                "ss_apply_t5_attn_mask",  # T5æ³¨æ„åŠ›æ©ç 
                "ss_sd_model_hash",  # SDæ¨¡å‹å“ˆå¸Œ
                "ss_training_comment",  # è®­ç»ƒæ³¨é‡Š
                # é¢å¤–çš„å†…éƒ¨å‚æ•°
                "ss_steps",  # å¦‚æœå’Œmax_train_stepsç›¸åŒå°±æ’é™¤
                "ss_num_epochs",  # å¦‚æœå’Œepochç›¸åŒå°±æ’é™¤
                "ss_v_parameterization",  # Vå‚æ•°åŒ–
                "ss_v_prediction",  # Vé¢„æµ‹
                "ss_dataset_split",  # æ•°æ®é›†åˆ†å‰²
                "ss_augmentation",  # å¢å¼º
                "ss_optimizer_args",  # ä¼˜åŒ–å™¨å‚æ•°
                "ss_network_args",  # ç½‘ç»œå‚æ•°
                "ss_max_bucket_resolution",  # æœ€å¤§åˆ†æ¡¶åˆ†è¾¨ç‡
                "ss_min_bucket_resolution",  # æœ€å°åˆ†æ¡¶åˆ†è¾¨ç‡
                "ss_cached_latents_epoch",  # ç¼“å­˜æ½œå˜é‡è½®æ•°
            }
            
            # åˆ†ç±»æ•´ç†è®­ç»ƒå‚æ•°
            network_params = {}  # ç½‘ç»œç»“æ„å‚æ•°
            training_params_basic = {}  # åŸºç¡€è®­ç»ƒå‚æ•°
            lr_params = {}  # å­¦ä¹ ç‡ç›¸å…³
            dataset_params = {}  # æ•°æ®é›†ç›¸å…³
            optimization_params = {}  # ä¼˜åŒ–ç›¸å…³
            other_params = {}  # å…¶ä»–å‚æ•°
            
            for k, v in metadata.items():
                if k.startswith("ss_") and k not in ["ss_tag_frequency", "ss_dataset_dirs"]:
                    # è·³è¿‡æ’é™¤çš„å‚æ•°
                    if k in excluded_params:
                        continue
                    
                    # ä½¿ç”¨åŸå§‹å‚æ•°åï¼Œå»æ‰ss_å‰ç¼€
                    param_name = k[3:] if k.startswith("ss_") else k
                    
                    # æ ¼å¼åŒ–å€¼
                    if isinstance(v, str):
                        # å¤„ç†å¸ƒå°”å€¼
                        if v.lower() == "true":
                            v = True
                        elif v.lower() == "false":
                            v = False
                        elif v.lower() == "none":
                            continue  # è·³è¿‡Noneå€¼
                        else:
                            # å°è¯•è§£ææ•°å­—
                            try:
                                if '.' in v:
                                    v = float(v)
                                    # å¦‚æœæ˜¯æ•´æ•°ï¼Œè½¬æ¢ä¸ºint
                                    if v.is_integer():
                                        v = int(v)
                                else:
                                    v = int(v)
                            except:
                                # å¤„ç†ç‰¹æ®Šæ ¼å¼
                                if v.startswith("(") and v.endswith(")") and "," in v:
                                    # å¤„ç†åˆ†è¾¨ç‡æ ¼å¼ "(1024, 1024)"
                                    try:
                                        v = v.strip("()").replace(" ", "")
                                    except:
                                        pass
                                # å¤„ç†ä¼˜åŒ–å™¨åç§°
                                elif "." in v and "optim" in v.lower():
                                    # ç®€åŒ–ä¼˜åŒ–å™¨åç§°
                                    parts = v.split(".")
                                    v = parts[-1] if parts else v
                    
                    # å¤„ç†æ—¶é—´æˆ³
                    if k in ["ss_training_started_at", "ss_training_finished_at"] and isinstance(v, (int, float)):
                        # è½¬æ¢Unixæ—¶é—´æˆ³ä¸ºå¯è¯»æ ¼å¼
                        import datetime
                        try:
                            dt = datetime.datetime.fromtimestamp(v)
                            v = dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            pass
                    
                    # æ ¼å¼åŒ–å­¦ä¹ ç‡
                    if k in ["ss_learning_rate", "ss_unet_lr", "ss_text_encoder_lr"]:
                        if isinstance(v, str):
                            try:
                                # å¤„ç†ç§‘å­¦è®¡æ•°æ³•
                                v = float(v)
                            except:
                                pass
                    
                    # åˆ†ç±»å‚æ•°
                    if k in ["ss_network_module", "ss_network_dim", "ss_network_alpha", "ss_base_model_version"]:
                        network_params[param_name] = v
                    elif k in ["ss_learning_rate", "ss_unet_lr", "ss_text_encoder_lr", "ss_lr_scheduler", "ss_lr_warmup_steps"]:
                        lr_params[param_name] = v
                    elif k in ["ss_max_train_steps", "ss_steps", "ss_epoch", "ss_num_epochs", "ss_batch_size_per_device", 
                              "ss_total_batch_size", "ss_gradient_accumulation_steps", "ss_seed", 
                              "ss_training_started_at", "ss_training_finished_at"]:
                        training_params_basic[param_name] = v
                    elif k in ["ss_resolution", "ss_enable_bucket", "ss_bucket_no_upscale", "ss_min_bucket_reso", 
                              "ss_max_bucket_reso", "ss_caption_dropout_rate", "ss_caption_tag_dropout_rate",
                              "ss_shuffle_caption", "ss_keep_tokens", "ss_max_token_length", "ss_cache_latents",
                              "ss_flip_aug", "ss_color_aug", "ss_random_crop", "ss_face_crop_aug_range"]:
                        dataset_params[param_name] = v
                    elif k in ["ss_optimizer", "ss_mixed_precision", "ss_gradient_checkpointing", "ss_clip_skip",
                              "ss_noise_offset", "ss_min_snr_gamma", "ss_scale_weight_norms", "ss_full_fp16",
                              "ss_fp8_base", "ss_fp8_base_unet", "ss_max_grad_norm", "ss_lowram"]:
                        optimization_params[param_name] = v
                    else:
                        # æ·»åŠ å…¶ä»–å‚æ•°
                        other_params[param_name] = v
            
            # æ„å»ºåˆ†å±‚çš„è®­ç»ƒå‚æ•°
            if network_params:
                training_params["Network Structure"] = network_params
            if training_params_basic:
                # åˆå¹¶é‡å¤çš„å‚æ•°
                # ä¿ç•™epochå‚æ•°ï¼Œå› ä¸ºå®ƒé€šå¸¸è¡¨ç¤ºå½“å‰è®­ç»ƒçš„epochæ•°ï¼Œä¸num_epochsï¼ˆæ€»epochæ•°ï¼‰ä¸åŒ
                # åªæœ‰å½“epochå’Œnum_epochså®Œå…¨ç›¸åŒæ—¶æ‰åˆ é™¤epoch
                if "epoch" in training_params_basic and "num_epochs" in training_params_basic:
                    if str(training_params_basic["epoch"]) == str(training_params_basic["num_epochs"]):
                        del training_params_basic["epoch"]
                    
                if "max_train_steps" in training_params_basic and "steps" in training_params_basic:
                    if str(training_params_basic["max_train_steps"]) == str(training_params_basic["steps"]):
                        del training_params_basic["steps"]
                        
                training_params["Basic Parameters"] = training_params_basic
            if lr_params:
                training_params["Learning Rate"] = lr_params
            if dataset_params:
                training_params["Dataset Settings"] = dataset_params
            if optimization_params:
                training_params["Optimization Settings"] = optimization_params
            if other_params:
                training_params["Other Parameters"] = other_params
                
            if training_params:
                full_metadata["è®­ç»ƒå‚æ•°"] = training_params

        # å¤„ç†æ ‡ç­¾é¢‘ç‡
        if include_tag_frequency and metadata and "ss_tag_frequency" in metadata:
            try:
                tag_frequency_raw = metadata["ss_tag_frequency"]
                if isinstance(tag_frequency_raw, str):
                    tag_frequency_data = json.loads(tag_frequency_raw)
                else:
                    tag_frequency_data = tag_frequency_raw
                
                # åˆå¹¶æ‰€æœ‰ç›®å½•çš„æ ‡ç­¾é¢‘ç‡
                all_tags = {}
                dir_tag_counts = {}  # è®°å½•æ¯ä¸ªç›®å½•çš„æ ‡ç­¾æ•°
                
                for dir_name, frequencies in tag_frequency_data.items():
                    dir_tag_counts[dir_name] = len(frequencies)
                    for tag, count in frequencies.items():
                        tag = tag.strip()
                        if tag in all_tags:
                            all_tags[tag] += count
                        else:
                            all_tags[tag] = count
                
                # æŒ‰é¢‘ç‡æ’åºå¹¶é™åˆ¶æ•°é‡
                if all_tags:
                    sorted_tags = dict(sorted(all_tags.items(), key=lambda x: x[1], reverse=True))
                    if len(sorted_tags) > max_tags:
                        sorted_tags = dict(list(sorted_tags.items())[:max_tags])
                    
                    # åˆ›å»ºæ ‡ç­¾é¢‘ç‡çš„å¯è§†åŒ–æ•°æ®
                    max_count = max(sorted_tags.values()) if sorted_tags else 1
                    
                    tag_freq_data = {
                        "æ ‡ç­¾æ€»æ•°": len(all_tags),
                        "æ˜¾ç¤ºæ•°é‡": len(sorted_tags),
                        "ç›®å½•æ•°": len(dir_tag_counts),
                    }
                    
                    # æ·»åŠ å‰Nä¸ªé«˜é¢‘æ ‡ç­¾
                    top_tags_list = []
                    for i, (tag, count) in enumerate(sorted_tags.items()):
                        if i < max_tags:  # ä½¿ç”¨max_tagså‚æ•°
                            percentage = (count / max_count) * 100
                            top_tags_list.append({
                                "æ ‡ç­¾": tag,
                                "å‡ºç°æ¬¡æ•°": count,
                                "é¢‘ç‡": f"{percentage:.1f}%"
                            })
                    
                    if top_tags_list:
                        tag_freq_data["é«˜é¢‘æ ‡ç­¾"] = top_tags_list
                    
                    # å¦‚æœæœ‰å¤šä¸ªç›®å½•ï¼Œæ˜¾ç¤ºæ¯ä¸ªç›®å½•çš„æ ‡ç­¾æ•°
                    if len(dir_tag_counts) > 1:
                        tag_freq_data["å„ç›®å½•æ ‡ç­¾æ•°"] = dir_tag_counts
                    
                    full_metadata["æ ‡ç­¾é¢‘ç‡åˆ†æ"] = tag_freq_data
                    
            except Exception as e:
                logging.warning(f"è§£ææ ‡ç­¾é¢‘ç‡å¤±è´¥: {e}")

        # å¤„ç†æ•°æ®é›†ä¿¡æ¯
        if include_dataset_info and metadata and "ss_dataset_dirs" in metadata:
            try:
                dataset_dirs_raw = metadata["ss_dataset_dirs"]
                if isinstance(dataset_dirs_raw, str):
                    dataset_dirs_data = json.loads(dataset_dirs_raw)
                else:
                    dataset_dirs_data = dataset_dirs_raw
                
                dataset_info = {
                    "overview": {},
                    "details": []
                }
                
                total_imgs = 0
                total_weighted = 0
                max_repeats = 0
                min_repeats = float('inf')
                
                for dir_name, counts in dataset_dirs_data.items():
                    img_count = int(counts.get("img_count", 0))
                    n_repeats = int(counts.get("n_repeats", 1))
                    weighted_total = img_count * n_repeats
                    
                    total_imgs += img_count
                    total_weighted += weighted_total
                    max_repeats = max(max_repeats, n_repeats)
                    min_repeats = min(min_repeats, n_repeats)
                    
                    # æå–ç›®å½•åä¸­çš„æ¦‚å¿µï¼ˆå¦‚æœæœ‰ï¼‰
                    dir_display = dir_name.split('\\')[-1].split('/')[-1]
                    
                    detail_item = {
                        "name": dir_display,
                        "image_count": img_count,
                        "repeats": f"Ã—{n_repeats}",
                        "weighted_total": weighted_total
                    }
                    
                    # åªæœ‰å½“è·¯å¾„å’Œæ˜¾ç¤ºåä¸åŒæ—¶æ‰æ·»åŠ åŸå§‹è·¯å¾„
                    if dir_name != dir_display and '\\' in dir_name or '/' in dir_name:
                        detail_item["original_path"] = dir_name
                        
                    dataset_info["details"].append(detail_item)
                
                # è®¡ç®—æ•°æ®é›†æ¦‚è§ˆ
                dataset_info["overview"] = {
                    "dataset_count": len(dataset_dirs_data),
                    "total_images": total_imgs,
                    "weighted_total_images": total_weighted,
                    "average_repeats": f"{total_weighted / total_imgs:.1f}" if total_imgs > 0 else "0",
                    "repeat_range": f"{min_repeats} - {max_repeats}" if min_repeats != float('inf') else "æ— "
                }
                
                # æŒ‰åŠ æƒæ€»æ•°æ’åº
                dataset_info["details"].sort(key=lambda x: x["weighted_total"], reverse=True)
                
                full_metadata["æ•°æ®é›†ä¿¡æ¯"] = dataset_info
                    
            except Exception as e:
                logging.warning(f"è§£ææ•°æ®é›†ä¿¡æ¯å¤±è´¥: {e}")

        # æ·»åŠ å…¶ä»–æœ‰ç”¨ä¿¡æ¯
        if metadata:
            # æ£€æŸ¥æ˜¯å¦ä¸ºFlux LoRA
            is_flux = False
            base_model = metadata.get("ss_base_model_version", "")
            if "flux" in base_model.lower():
                is_flux = True
            elif metadata.get("ss_sd_model_name", ""):
                if "flux" in metadata.get("ss_sd_model_name", "").lower():
                    is_flux = True
            
            # æ·»åŠ æ¨¡å‹ç±»å‹ä¿¡æ¯
            model_info = {
                "æ¨¡å‹ç±»å‹": "Flux LoRA" if is_flux else "LoRA",
                "æ–‡ä»¶å¤§å°": f"{os.path.getsize(file_path) / (1024*1024):.1f} MB" if os.path.exists(file_path) else "æœªçŸ¥",
            }
            
            # æ·»åŠ è§¦å‘è¯ï¼ˆå¦‚æœæœ‰ï¼‰
            trigger_words = []
            if metadata.get("ss_trigger_words"):
                trigger_words = metadata.get("ss_trigger_words", "").split(",")
            elif metadata.get("trigger_words"):
                trigger_words = metadata.get("trigger_words", "").split(",")
                
            if trigger_words:
                model_info["è§¦å‘è¯"] = [word.strip() for word in trigger_words if word.strip()]
                
            full_metadata["æ¨¡å‹ä¿¡æ¯"] = model_info
            
            # æ·»åŠ æœªåˆ†ç±»çš„å…¶ä»–å…ƒæ•°æ®ï¼ˆæ’é™¤å·²å¤„ç†çš„å’Œå†…éƒ¨ä½¿ç”¨çš„ï¼‰
            excluded_keys = {
                "ss_tag_frequency", "ss_dataset_dirs", "ssmd_display_name", "ssmd_author",
                "ssmd_description", "ssmd_keywords", "ssmd_tags", "ssmd_rating", "ssmd_source",
                "ssmd_version", "sshs_model_hash", "sshs_legacy_hash", "ss_trigger_words",
                "trigger_words"
            }
            
            # å¤„ç†ModelSpecå…ƒæ•°æ®ï¼Œå»æ‰å‰ç¼€
            
            modelspec_data = {}
            other_metadata = {}
            
            for k, v in metadata.items():
                if not k.startswith("ss_") and k not in excluded_keys:
                    # åªæ·»åŠ æœ‰æ„ä¹‰çš„å€¼
                    if v and str(v).strip() and str(v) not in ["None", "null", "undefined", ""]:
                        # å¤„ç†ModelSpecå…ƒæ•°æ®
                        if k.startswith("modelspec."):
                            # å»æ‰modelspec.å‰ç¼€
                            spec_name = k.replace("modelspec.", "")
                            modelspec_data[spec_name] = v
                        else:
                            other_metadata[k] = v
            
            # æ·»åŠ ModelSpecä¿¡æ¯
            if modelspec_data:
                full_metadata["æ¨¡å‹è§„èŒƒ"] = modelspec_data
                        
            # åªæœ‰å½“æœ‰å…¶ä»–å…ƒæ•°æ®æ—¶æ‰æ·»åŠ 
            if other_metadata:
                full_metadata["å…¶ä»–å…ƒæ•°æ®"] = other_metadata

        logging.info(f"æˆåŠŸè¯»å–LoRAå…ƒæ•°æ®: {model_name}")
        
        # æ ¼å¼åŒ–è¾“å‡ºä¸ºå¯è¯»çš„å­—ç¬¦ä¸²
        output_lines = []
        
        # åŸºæœ¬ä¿¡æ¯
        output_lines.append("ã€Basic Info / åŸºæœ¬ä¿¡æ¯ã€‘")
        output_lines.append(f"Model Name: {model_name}")
        if basic_info:
            if "æ¨¡å‹å“ˆå¸Œ" in basic_info:
                output_lines.append(f"Model Hash: {basic_info['æ¨¡å‹å“ˆå¸Œ']}")
            if "ä¼ ç»Ÿå“ˆå¸Œ" in basic_info:
                output_lines.append(f"Legacy Hash: {basic_info['ä¼ ç»Ÿå“ˆå¸Œ']}")
            if "ä½œè€…" in basic_info:
                output_lines.append(f"Author: {basic_info['ä½œè€…']}")
            if "ç‰ˆæœ¬" in basic_info:
                output_lines.append(f"Version: {basic_info['ç‰ˆæœ¬']}")
        
        # æ¨¡å‹ä¿¡æ¯
        if "æ¨¡å‹ä¿¡æ¯" in full_metadata:
            output_lines.append("\nã€Model Info / æ¨¡å‹ä¿¡æ¯ã€‘")
            model_info = full_metadata["æ¨¡å‹ä¿¡æ¯"]
            output_lines.append(f"Type: {model_info.get('æ¨¡å‹ç±»å‹', 'æœªçŸ¥')}")
            output_lines.append(f"File Size: {model_info.get('æ–‡ä»¶å¤§å°', 'æœªçŸ¥')}")
            if "è§¦å‘è¯" in model_info:
                output_lines.append(f"Trigger Words: {', '.join(model_info['è§¦å‘è¯'])}")
        
        # è®­ç»ƒå‚æ•°ï¼ˆæ˜¾ç¤ºå®Œæ•´ä¿¡æ¯ï¼‰
        if "è®­ç»ƒå‚æ•°" in full_metadata:
            output_lines.append("\nã€Training Parameters / è®­ç»ƒå‚æ•°ã€‘")
            training = full_metadata["è®­ç»ƒå‚æ•°"]
            
            # ç½‘ç»œç»“æ„
            if "Network Structure" in training:
                net = training["Network Structure"]
                output_lines.append("â—† Network Structure / ç½‘ç»œç»“æ„")
                for k, v in net.items():
                    output_lines.append(f"  - {k}: {v}")
            
            # åŸºç¡€å‚æ•°
            if "Basic Parameters" in training:
                basic = training["Basic Parameters"]
                output_lines.append("â—† Basic Parameters / åŸºç¡€å‚æ•°")
                for k, v in basic.items():
                    output_lines.append(f"  - {k}: {v}")
            
            # å­¦ä¹ ç‡
            if "Learning Rate" in training:
                lr = training["Learning Rate"]
                output_lines.append("â—† Learning Rate / å­¦ä¹ ç‡")
                for k, v in lr.items():
                    output_lines.append(f"  - {k}: {v}")
            
            # æ•°æ®é›†è®¾ç½®
            if "Dataset Settings" in training:
                ds_settings = training["Dataset Settings"]
                output_lines.append("â—† Dataset Settings / æ•°æ®é›†è®¾ç½®")
                for k, v in ds_settings.items():
                    output_lines.append(f"  - {k}: {v}")
            
            # ä¼˜åŒ–è®¾ç½®
            if "Optimization Settings" in training:
                opt_settings = training["Optimization Settings"]
                output_lines.append("â—† Optimization Settings / ä¼˜åŒ–è®¾ç½®")
                for k, v in opt_settings.items():
                    output_lines.append(f"  - {k}: {v}")
            
            # å…¶ä»–å‚æ•°
            if "Other Parameters" in training:
                other_params = training["Other Parameters"]
                output_lines.append("â—† Other Parameters / å…¶ä»–å‚æ•°")
                for k, v in other_params.items():
                    output_lines.append(f"  - {k}: {v}")
        
        # æ•°æ®é›†ä¿¡æ¯ï¼ˆå®Œæ•´æ˜¾ç¤ºï¼‰
        if "æ•°æ®é›†ä¿¡æ¯" in full_metadata:
            dataset = full_metadata["æ•°æ®é›†ä¿¡æ¯"]
            output_lines.append("\nã€Dataset Info / æ•°æ®é›†ä¿¡æ¯ã€‘")
            
            if "overview" in dataset:
                overview = dataset["overview"]
                output_lines.append("â—† Overview / æ¦‚è§ˆ")
                for k, v in overview.items():
                    output_lines.append(f"  - {k}: {v}")
            
            if "details" in dataset and len(dataset["details"]) > 0:
                output_lines.append("â—† Dataset Details / æ•°æ®é›†è¯¦æƒ…")
                for ds in dataset["details"]:  # æ˜¾ç¤ºæ‰€æœ‰æ•°æ®é›†
                    ds_line = f"  - {ds['name']}: {ds['image_count']} images {ds['repeats']} (weighted: {ds['weighted_total']})"
                    if "original_path" in ds:
                        ds_line += f"\n    path: {ds['original_path']}"
                    output_lines.append(ds_line)
        
        # æ ‡ç­¾é¢‘ç‡ï¼ˆæ˜¾ç¤ºå‰50ä¸ªï¼‰
        if "æ ‡ç­¾é¢‘ç‡åˆ†æ" in full_metadata:
            tag_analysis = full_metadata["æ ‡ç­¾é¢‘ç‡åˆ†æ"]
            output_lines.append("\nã€Tag Frequency / æ ‡ç­¾é¢‘ç‡ã€‘")
            output_lines.append(f"Total Tags: {tag_analysis.get('æ ‡ç­¾æ€»æ•°', 0)} | Showing: {tag_analysis.get('æ˜¾ç¤ºæ•°é‡', 0)}")
            
            # å„ç›®å½•æ ‡ç­¾æ•°
            if "å„ç›®å½•æ ‡ç­¾æ•°" in tag_analysis:
                output_lines.append("â—† Tags per Directory / å„ç›®å½•æ ‡ç­¾æ•°")
                for dir_name, count in tag_analysis["å„ç›®å½•æ ‡ç­¾æ•°"].items():
                    output_lines.append(f"  - {dir_name}: {count} tags")
            
            # é«˜é¢‘æ ‡ç­¾åˆ—è¡¨
            if "é«˜é¢‘æ ‡ç­¾" in tag_analysis:
                output_lines.append("â—† Top Tags / é«˜é¢‘æ ‡ç­¾")
                for i, tag in enumerate(tag_analysis["é«˜é¢‘æ ‡ç­¾"][:50]):  # æ˜¾ç¤ºå‰50ä¸ª
                    output_lines.append(f"  {i+1}. {tag['æ ‡ç­¾']} ({tag['é¢‘ç‡']})")
        
        # æ¨¡å‹è§„èŒƒ
        if "æ¨¡å‹è§„èŒƒ" in full_metadata:
            spec = full_metadata["æ¨¡å‹è§„èŒƒ"]
            output_lines.append("\nã€Model Specification / æ¨¡å‹è§„èŒƒã€‘")
            for k, v in spec.items():
                output_lines.append(f"  - {k}: {v}")
        
        # å…¶ä»–å…ƒæ•°æ®
        if "å…¶ä»–å…ƒæ•°æ®" in full_metadata:
            other = full_metadata["å…¶ä»–å…ƒæ•°æ®"]
            output_lines.append("\nã€Other Metadata / å…¶ä»–å…ƒæ•°æ®ã€‘")
            for k, v in other.items():
                output_lines.append(f"  - {k}: {v}")
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æ•°æ®
        if len(output_lines) == 0:
            output_lines.append("æœªæ‰¾åˆ°æœ‰æ•ˆçš„å…ƒæ•°æ®")
        
        return ("\n".join(output_lines),) 