import torch
import numpy as np
from PIL import Image

class Transparent_Area_Cropper:
    """
    æ ¹æ®maskå…ƒç´ ä½ç½®è£å‰ªç”»å¸ƒï¼Œç±»ä¼¼PSçš„ç”»å¸ƒå¤§å°è°ƒæ•´åŠŸèƒ½ã€‚
    æ”¯æŒç™½åº•é»‘å…ƒç´ çš„maskæ ¼å¼ã€‚
    æ­£æ–¹å½¢æ¨¡å¼ï¼šæ ¹æ®è£å‰ªåŒºåŸŸçš„æœ€å¤§ç»´åº¦åˆ›å»ºæ­£æ–¹å½¢ç”»å¸ƒã€‚
    çŸ©å½¢æ¨¡å¼ï¼šæ ¹æ®maskè¾¹ç•Œæ¡†ç²¾ç¡®è£å‰ªç”»å¸ƒã€‚
    æ³¨æ„ï¼šåªè£å‰ªç”»å¸ƒå¤§å°ï¼Œä¸ä¿®æ”¹å›¾åƒå†…å®¹æœ¬èº«ã€‚
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾ç‰‡": ("IMAGE",),
                "è’™ç‰ˆ": ("MASK",),
                "æ­£æ–¹å½¢è¾“å‡º": ("BOOLEAN", {
                    "default": True,
                    "display": "checkbox"
                }),
                "ç™¾åˆ†æ¯”æ‰©å±•": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 200,
                    "step": 1,
                    "display": "number"
                }),
                "æœ€å°æ‰©å±•åƒç´ ": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                    "display": "number"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "IMAGE", "MASK",)
    RETURN_NAMES = ("åˆå¹¶Alphaå›¾ç‰‡", "è£å‰ªåå›¾ç‰‡", "é€æ˜è’™ç‰ˆ",)
    FUNCTION = "crop_transparent_area"
    CATEGORY = "ğŸ’ Kim-Nodes/âœ‚ Crop | è£å‰ªå·¥å…·"

    def tensor2pil(self, image):
        # å¦‚æœè¾“å…¥æ˜¯(batch, height, width, channels)æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªæ ·æœ¬
        if len(image.shape) == 4:
            image = image[0]
        return Image.fromarray(np.clip(255. * image.cpu().numpy(), 0, 255).astype(np.uint8))

    def pil2tensor(self, image):
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–
        img_tensor = torch.from_numpy(np.array(image).astype(np.float32) / 255.0)
        # æ·»åŠ batchç»´åº¦
        img_tensor = img_tensor.unsqueeze(0)
        return img_tensor
    
    def create_masked_alpha_image(self, image, mask):
        """
        ä½¿ç”¨maskåˆ›å»ºé€æ˜åº•å›¾åƒ
        maskä¸­é»‘è‰²åŒºåŸŸ(0)ä¿ç•™å›¾åƒå†…å®¹ï¼Œç™½è‰²åŒºåŸŸ(255)å˜ä¸ºé€æ˜
        """
        # ç¡®ä¿å›¾åƒæ˜¯RGBAæ ¼å¼
        if image.mode != 'RGBA':
            image = image.convert('RGBA')
        
        # è·å–å›¾åƒå’Œmaskçš„æ•°ç»„
        img_array = np.array(image)
        mask_array = np.array(mask)
        
        # åˆ›å»ºç»“æœæ•°ç»„
        result_array = img_array.copy()
        
        # maskä¸­ç™½è‰²åŒºåŸŸ(255)å¯¹åº”é€æ˜ï¼Œé»‘è‰²åŒºåŸŸ(0)ä¿ç•™
        # æ‰€ä»¥æˆ‘ä»¬éœ€è¦åè½¬maskï¼šç™½è‰²->é€æ˜ï¼Œé»‘è‰²->ä¸é€æ˜
        alpha_mask = 255 - mask_array  # åè½¬mask
        
        # å°†åè½¬åçš„maskåº”ç”¨åˆ°alphaé€šé“
        result_array[:, :, 3] = np.minimum(result_array[:, :, 3], alpha_mask)
        
        return Image.fromarray(result_array, 'RGBA')

    def crop_transparent_area(self, å›¾ç‰‡, è’™ç‰ˆ, æ­£æ–¹å½¢è¾“å‡º=True, ç™¾åˆ†æ¯”æ‰©å±•=0.0, æœ€å°æ‰©å±•åƒç´ =0):
        print(f"è¾“å…¥å›¾ç‰‡ç»´åº¦: {å›¾ç‰‡.shape}")
        print(f"è¾“å…¥è’™ç‰ˆç»´åº¦: {è’™ç‰ˆ.shape}")
        
        # å¤„ç†è¾“å…¥å›¾ç‰‡
        if isinstance(å›¾ç‰‡, torch.Tensor):
            if len(å›¾ç‰‡.shape) == 4:
                img_tensor = å›¾ç‰‡[0]
            else:
                img_tensor = å›¾ç‰‡
            # è½¬æ¢ä¸ºPILå›¾åƒ
            pil_img = self.tensor2pil(img_tensor)
        else:
            pil_img = å›¾ç‰‡
            
        # ç¡®ä¿å›¾åƒæœ‰é€æ˜é€šé“
        if pil_img.mode != 'RGBA':
            if pil_img.mode == 'RGB':
                pil_img = pil_img.convert('RGBA')
            else:
                pil_img = pil_img.convert('RGBA')
        
        # å¤„ç†è’™ç‰ˆ
        if isinstance(è’™ç‰ˆ, torch.Tensor):
            # å¤„ç†ä¸åŒç»´åº¦çš„è’™ç‰ˆ
            if len(è’™ç‰ˆ.shape) == 4:  # (batch, channels, height, width)
                mask_np = è’™ç‰ˆ[0, 0].cpu().numpy()
            elif len(è’™ç‰ˆ.shape) == 3:  # (batch, height, width) æˆ– (channels, height, width)
                if è’™ç‰ˆ.shape[0] == 1:
                    mask_np = è’™ç‰ˆ[0].cpu().numpy()
                else:
                    mask_np = è’™ç‰ˆ[0].cpu().numpy()
            elif len(è’™ç‰ˆ.shape) == 2:  # (height, width)
                mask_np = è’™ç‰ˆ.cpu().numpy()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è’™ç‰ˆç»´åº¦: {è’™ç‰ˆ.shape}")
            
            # å°†è’™ç‰ˆè½¬æ¢ä¸ºäºŒå€¼å›¾åƒï¼ˆ0æˆ–255ï¼‰
            mask_np = (mask_np * 255).astype(np.uint8)
        else:
            mask_np = np.array(è’™ç‰ˆ)
        
        # å¯¹äºç™½åº•é»‘å…ƒç´ çš„maskï¼Œéœ€è¦åè½¬ï¼šé»‘è‰²åŒºåŸŸ(0)æ˜¯æœ‰æ•ˆåŒºåŸŸï¼Œç™½è‰²åŒºåŸŸ(255)æ˜¯èƒŒæ™¯
        # åè½¬maskï¼šé»‘è‰²å˜ç™½è‰²ï¼Œç™½è‰²å˜é»‘è‰²
        inverted_mask_np = 255 - mask_np
        
        print(f"åŸå§‹maskç»Ÿè®¡: æœ€å°å€¼={mask_np.min()}, æœ€å¤§å€¼={mask_np.max()}")
        print(f"åè½¬maskç»Ÿè®¡: æœ€å°å€¼={inverted_mask_np.min()}, æœ€å¤§å€¼={inverted_mask_np.max()}")
        
        # ä»åè½¬åçš„è’™ç‰ˆè·å–éé›¶åŒºåŸŸçš„è¾¹ç•Œæ¡†ï¼ˆç°åœ¨é»‘è‰²å…ƒç´ å˜æˆäº†ç™½è‰²ï¼‰
        mask_pil = Image.fromarray(mask_np, mode='L')  # ä¿æŒåŸå§‹maskç”¨äºåç»­å¤„ç†
        inverted_mask_pil = Image.fromarray(inverted_mask_np, mode='L')  # åè½¬maskç”¨äºè·å–è¾¹ç•Œæ¡†
        bbox = inverted_mask_pil.getbbox()  # è¿”å›(left, upper, right, lower)
        
        if not bbox:
            print("è’™ç‰ˆä¸­æœªæ£€æµ‹åˆ°éé›¶åŒºåŸŸï¼Œè¿”å›ç©ºå›¾")
            # åˆ›å»º1x1çš„é€æ˜å›¾ç‰‡
            empty_img = Image.new('RGBA', (1, 1), (0, 0, 0, 0))
            empty_tensor = self.pil2tensor(empty_img)
            empty_mask = torch.zeros((1, 1, 1)).float()
            # åˆ›å»º1x1çš„é€æ˜å›¾ç‰‡ä½œä¸ºalphaåˆæˆç»“æœ
            empty_composite = Image.new('RGBA', (1, 1), (0, 0, 0, 0))
            empty_composite_tensor = self.pil2tensor(empty_composite)
            return (empty_composite_tensor, empty_tensor, empty_mask,)
        
        # è§£åŒ…è¾¹ç•Œæ¡† - è¿™å°±æ˜¯maskçš„æœ‰æ•ˆåŒºåŸŸ
        left, top, right, bottom = bbox
        
        # è®¡ç®—åŸå§‹maskåŒºåŸŸå°ºå¯¸
        orig_width = right - left
        orig_height = bottom - top
        
        # åº”ç”¨æ‰©å±•å‚æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if ç™¾åˆ†æ¯”æ‰©å±• != 0 or æœ€å°æ‰©å±•åƒç´  != 0:
            # è®¡ç®—ç™¾åˆ†æ¯”æ‰©å±•åƒç´ 
            width_expand = max(int(orig_width * (ç™¾åˆ†æ¯”æ‰©å±• / 100.0)), æœ€å°æ‰©å±•åƒç´  if ç™¾åˆ†æ¯”æ‰©å±• > 0 else 0)
            height_expand = max(int(orig_height * (ç™¾åˆ†æ¯”æ‰©å±• / 100.0)), æœ€å°æ‰©å±•åƒç´  if ç™¾åˆ†æ¯”æ‰©å±• > 0 else 0)
            
            # å¦‚æœç™¾åˆ†æ¯”ä¸ºè´Ÿï¼Œåˆ™æ”¶ç¼©è€Œä¸æ˜¯æ‰©å±•
            if ç™¾åˆ†æ¯”æ‰©å±• < 0:
                width_expand = int(orig_width * (ç™¾åˆ†æ¯”æ‰©å±• / 100.0))
                height_expand = int(orig_height * (ç™¾åˆ†æ¯”æ‰©å±• / 100.0))
            
            # è®¡ç®—æ‰©å±•åçš„è¾¹ç•Œ
            crop_left = left - width_expand
            crop_top = top - height_expand
            crop_right = right + width_expand
            crop_bottom = bottom + height_expand
        else:
            # å¦‚æœæ²¡æœ‰æ‰©å±•ï¼Œç›´æ¥ä½¿ç”¨maskçš„è¾¹ç•Œæ¡†
            crop_left = left
            crop_top = top
            crop_right = right
            crop_bottom = bottom
        
        # é™åˆ¶è£å‰ªåŒºåŸŸåœ¨åŸå›¾èŒƒå›´å†…
        crop_left = max(0, crop_left)
        crop_top = max(0, crop_top)
        crop_right = min(pil_img.width, crop_right)
        crop_bottom = min(pil_img.height, crop_bottom)
        
        # è®¡ç®—å®é™…è£å‰ªå°ºå¯¸
        crop_width = crop_right - crop_left
        crop_height = crop_bottom - crop_top
        
        # ç¡®ä¿å°ºå¯¸ä¸ä¼šå°äº1
        crop_width = max(1, crop_width)
        crop_height = max(1, crop_height)
        
        print(f"æ£€æµ‹åˆ°maskå…ƒç´ è¾¹ç•Œ: å·¦({left}), å³({right}), ä¸Š({top}), ä¸‹({bottom})")
        print(f"maskå…ƒç´ å°ºå¯¸: {orig_width}x{orig_height}")
        if ç™¾åˆ†æ¯”æ‰©å±• != 0 or æœ€å°æ‰©å±•åƒç´  != 0:
            print(f"ç”»å¸ƒæ‰©å±•å‚æ•°: ç™¾åˆ†æ¯”={ç™¾åˆ†æ¯”æ‰©å±•}%, æœ€å°åƒç´ ={æœ€å°æ‰©å±•åƒç´ }")
        print(f"ç”»å¸ƒè£å‰ªåŒºåŸŸ: ({crop_left}, {crop_top}) åˆ° ({crop_right}, {crop_bottom})")
        print(f"ç”»å¸ƒè£å‰ªå°ºå¯¸: {crop_width}x{crop_height}")
        
        # ç¡®ä¿è£å‰ªåŒºåŸŸæœ‰æ•ˆ
        if crop_left >= crop_right or crop_top >= crop_bottom:
            print("æ— æœ‰æ•ˆè£å‰ªåŒºåŸŸï¼Œè¿”å›ç©ºå›¾")
            empty_img = Image.new('RGBA', (1, 1), (0, 0, 0, 0))
            empty_tensor = self.pil2tensor(empty_img)
            empty_mask = torch.zeros((1, 1, 1)).float()
            # åˆ›å»º1x1çš„é€æ˜å›¾ç‰‡ä½œä¸ºalphaåˆæˆç»“æœ
            empty_composite = Image.new('RGBA', (1, 1), (0, 0, 0, 0))
            empty_composite_tensor = self.pil2tensor(empty_composite)
            return (empty_composite_tensor, empty_tensor, empty_mask,)
        
        # è£å‰ªæºå›¾åƒå’Œè’™ç‰ˆ
        cropped_src = pil_img.crop((crop_left, crop_top, crop_right, crop_bottom))
        # ä½¿ç”¨åè½¬åçš„maskè¿›è¡Œè£å‰ªï¼Œè¿™æ ·é»‘è‰²å…ƒç´ åŒºåŸŸå˜æˆç™½è‰²ï¼ˆæœ‰æ•ˆåŒºåŸŸï¼‰
        cropped_mask = inverted_mask_pil.crop((crop_left, crop_top, crop_right, crop_bottom))
        
        # ç¡®ä¿å›¾åƒå’Œè’™ç‰ˆå°ºå¯¸ä¸€è‡´
        if cropped_src.size != cropped_mask.size:
            min_width = min(cropped_src.width, cropped_mask.width)
            min_height = min(cropped_src.height, cropped_mask.height)
            cropped_src = cropped_src.crop((0, 0, min_width, min_height))
            cropped_mask = cropped_mask.crop((0, 0, min_width, min_height))
        
        # ç”»å¸ƒè£å‰ªæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨è£å‰ªåçš„å›¾åƒï¼Œä¸åº”ç”¨maskå¤„ç†
        # ç¡®ä¿å›¾åƒæ˜¯RGBAæ ¼å¼ä»¥ä¿æŒé€æ˜åº¦
        if cropped_src.mode != 'RGBA':
            if cropped_src.mode == 'RGB':
                cropped_src = cropped_src.convert('RGBA')
            else:
                cropped_src = cropped_src.convert('RGBA')
        
        # ç›´æ¥ä½¿ç”¨è£å‰ªåçš„å›¾åƒï¼Œä¿æŒåŸå§‹å†…å®¹å’Œé€æ˜åº¦
        canvas_cropped_img = cropped_src
        
        # ä¸ºäº†è¾“å‡ºmaskï¼Œæˆ‘ä»¬éœ€è¦å°†åè½¬çš„maskå†åè½¬å›æ¥ï¼Œä¿æŒä¸è¾“å…¥ç›¸åŒçš„æ ¼å¼ï¼ˆç™½åº•é»‘å…ƒç´ ï¼‰
        cropped_original_mask = mask_pil.crop((crop_left, crop_top, crop_right, crop_bottom))
        
        # æ ¹æ®è¾“å‡ºæ¨¡å¼å¤„ç†æœ€ç»ˆç»“æœ
        if æ­£æ–¹å½¢è¾“å‡º:
            # åˆ›å»ºæ­£æ–¹å½¢è¾“å‡º - ä½¿ç”¨è£å‰ªåçš„æœ€å¤§ç»´åº¦
            max_dim = max(canvas_cropped_img.width, canvas_cropped_img.height)
            output_img = Image.new('RGBA', (max_dim, max_dim), (0, 0, 0, 0))
            # è®¡ç®—å±…ä¸­ä½ç½®
            center_x = (max_dim - canvas_cropped_img.width) // 2
            center_y = (max_dim - canvas_cropped_img.height) // 2
            output_img.paste(canvas_cropped_img, (center_x, center_y), canvas_cropped_img)
            
            # åˆ›å»ºæ­£æ–¹å½¢è’™ç‰ˆ - ä½¿ç”¨åŸå§‹æ ¼å¼ï¼ˆç™½åº•é»‘å…ƒç´ ï¼‰
            output_mask_img = Image.new('L', (max_dim, max_dim), 255)  # ç™½è‰²èƒŒæ™¯
            output_mask_img.paste(cropped_original_mask, (center_x, center_y))
            
            # åˆ›å»ºé€æ˜åº•çš„alphaåˆæˆå›¾åƒ - ä½¿ç”¨maskæ¥æ§åˆ¶é€æ˜åº¦
            alpha_composite_img = self.create_masked_alpha_image(output_img, output_mask_img)
            
            print(f"æ­£æ–¹å½¢ç”»å¸ƒè¾“å‡ºå°ºå¯¸: {max_dim}x{max_dim}")
        else:
            # ç›´æ¥ä½¿ç”¨è£å‰ªåçš„ç»“æœ
            output_img = canvas_cropped_img
            output_mask_img = cropped_original_mask  # ä½¿ç”¨åŸå§‹æ ¼å¼çš„mask
            
            # åˆ›å»ºé€æ˜åº•çš„alphaåˆæˆå›¾åƒ - ä½¿ç”¨maskæ¥æ§åˆ¶é€æ˜åº¦
            alpha_composite_img = self.create_masked_alpha_image(output_img, output_mask_img)
            
            print(f"çŸ©å½¢ç”»å¸ƒè¾“å‡ºå°ºå¯¸: {canvas_cropped_img.width}x{canvas_cropped_img.height}")
        
        # è½¬æ¢å›tensor
        output_tensor = self.pil2tensor(output_img)
        
        # è½¬æ¢è¾“å‡ºè’™ç‰ˆä¸ºtensor
        mask_array = np.array(output_mask_img).astype(np.float32) / 255.0
        mask_tensor = torch.from_numpy(mask_array)
        mask_tensor = mask_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        # è½¬æ¢alphaåˆæˆå›¾åƒä¸ºtensor
        alpha_composite_tensor = self.pil2tensor(alpha_composite_img)
        
        return (alpha_composite_tensor, output_tensor, mask_tensor,) 