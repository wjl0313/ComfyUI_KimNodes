import numpy as np
import cv2
import torch

class KimHDR:
    """
    ä¸€ä¸ªå›¾åƒå¤„ç†èŠ‚ç‚¹ï¼Œå¯¹å›¾åƒåº”ç”¨æœ€å…ˆè¿›çš„HDRç®—æ³•ã€‚
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE", {
                    "description": "ä¸Šä¼ æ‚¨æƒ³åº”ç”¨é«˜çº§HDRå¤„ç†æ•ˆæœçš„å›¾åƒã€‚"
                }),
                "HDRå¼ºåº¦": ("FLOAT", {
                    "default": 1,
                    "min": 0.5,
                    "max": 3.0,
                    "step": 0.01,
                    "description": "HDRå¼ºåº¦ï¼Œä»0åˆ°3ã€‚"
                }),
                "æ¬ æ›å…‰å› å­": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "description": "æ¬ æ›å…‰å› å­ï¼Œä»0åˆ°1ã€‚"
                }),
                "è¿‡æ›å…‰å› å­": ("FLOAT", {
                    "default": 1,
                    "min": 1.0,
                    "max": 2.0,
                    "step": 0.01,
                    "description": "è¿‡æ›å…‰å› å­ï¼Œä»1åˆ°2ã€‚"
                }),
                "gamma": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.1,
                    "max": 3.0,
                    "step": 0.01,
                    "description": "è‰²è°ƒæ˜ å°„å™¨çš„gammaå€¼ï¼Œä»0.1åˆ°3.0ã€‚"
                }),
                "é«˜å…‰ç»†èŠ‚": ("FLOAT", {
                    "default": 1/30.0,
                    "min": 1/1000.0,
                    "max": 1.0,
                    "step": 0.01,
                    "description": "é«˜å…‰ç»†èŠ‚ã€‚"
                }),
                "ä¸­é—´è°ƒç»†èŠ‚": ("FLOAT", {
                    "default": 0.25,
                    "min": 1/1000.0,
                    "max": 1.0,
                    "step": 0.01,
                    "description": "ä¸­é—´è°ƒç»†èŠ‚ã€‚"
                }),
                "é˜´å½±ç»†èŠ‚": ("FLOAT", {
                    "default": 2,
                    "min": 1/1000.0,
                    "max": 10.0,
                    "step": 0.1,
                    "description": "é˜´å½±ç»†èŠ‚ã€‚"
                }),
                "æ•´ä½“å¼ºåº¦": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.01,
                    "description": "å¤„ç†æ•ˆæœå¯¹æœ€ç»ˆå›¾åƒçš„å½±å“ç¨‹åº¦ï¼Œä»0åˆ°1ã€‚"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "execute"
    CATEGORY = "ğŸŠ Kim-Nodes/ğŸ¨Filter | æ»¤é•œ"

    def execute(self, image, HDRå¼ºåº¦, æ¬ æ›å…‰å› å­, è¿‡æ›å…‰å› å­, gamma, é«˜å…‰ç»†èŠ‚, ä¸­é—´è°ƒç»†èŠ‚, é˜´å½±ç»†èŠ‚, æ•´ä½“å¼ºåº¦):
        try:

            image = self.ensure_image_format(image)

            processed_image = self.apply_hdr(image, HDRå¼ºåº¦, æ¬ æ›å…‰å› å­, è¿‡æ›å…‰å› å­, gamma, [é«˜å…‰ç»†èŠ‚, ä¸­é—´è°ƒç»†èŠ‚, é˜´å½±ç»†èŠ‚])

            # æ··åˆåŸå§‹å›¾åƒå’Œå¤„ç†åçš„å›¾åƒ
            blended_image = cv2.addWeighted(processed_image, æ•´ä½“å¼ºåº¦, image, 1 - æ•´ä½“å¼ºåº¦, 0)

            if isinstance(blended_image, np.ndarray):
                blended_image = np.expand_dims(blended_image, axis=0)

            blended_image = torch.from_numpy(blended_image).float()
            blended_image = blended_image / 255.0
            blended_image = blended_image.to(torch.device('cpu'))

            return [blended_image]
        except Exception as e:
            if image is not None and hasattr(image, 'shape'):
                black_image = torch.zeros((1, 3, image.shape[0], image.shape[1]), dtype=torch.float32)
            else:
                black_image = torch.zeros((1, 3, 224, 224), dtype=torch.float32)
            return [black_image.to(torch.device('cpu'))]

    def ensure_image_format(self, image):
        if isinstance(image, torch.Tensor):
            if image.dim() == 4:
                image = image.squeeze(0)
            image = image.numpy() * 255
            image = image.astype(np.uint8)
        return image

    def apply_hdr(self, image, HDRå¼ºåº¦, æ¬ æ›å…‰å› å­, è¿‡æ›å…‰å› å­, gamma, æ›å…‰æ—¶é—´):
        # åˆ›å»ºHDRåˆæˆå™¨
        hdr = cv2.createMergeDebevec()

        # æ›å…‰æ—¶é—´
        times = np.array(æ›å…‰æ—¶é—´, dtype=np.float32)

        # ç”Ÿæˆä¸åŒæ›å…‰çš„å›¾åƒ
        exposure_images = [
            np.clip(image * æ¬ æ›å…‰å› å­, 0, 255).astype(np.uint8),  # æ¬ æ›å…‰
            image,  # æ­£å¸¸æ›å…‰
            np.clip(image * è¿‡æ›å…‰å› å­, 0, 255).astype(np.uint8)   # è¿‡æ›å…‰
        ]

        # åˆæˆHDRå›¾åƒ
        hdr_image = hdr.process(exposure_images, times=times.copy())

        # ä½¿ç”¨è‰²è°ƒæ˜ å°„å™¨
        tonemap = cv2.createTonemapReinhard(gamma=gamma)
        ldr_image = tonemap.process(hdr_image)

        # è°ƒæ•´HDRå¼ºåº¦
        ldr_image = ldr_image * HDRå¼ºåº¦
        ldr_image = np.clip(ldr_image, 0, 1)

        # è½¬æ¢ä¸º8ä½å›¾åƒ
        ldr_image = np.clip(ldr_image * 255, 0, 255).astype(np.uint8)

        return ldr_image
