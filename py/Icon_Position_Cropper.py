import torch
import numpy as np
from PIL import Image

class IconPositionCropper:
    """
    æ ¹æ®æŒ‡å®šçš„å››ä¸ªåæ ‡ç‚¹æ¥è£åˆ‡å›¾ç‰‡
    """
    
    def __init__(self):
        pass
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾ç‰‡": ("IMAGE",),
                "ä½ç½®æ•°æ®": ("DATA",),
                "èµ·å§‹åˆ—å·": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                }),
                "ç»ˆæ­¢åˆ—å·": ("INT", {
                    "default": 8,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                }),
                "è¡Œå·": ("INT", {
                    "default": 8,
                    "min": 0,
                    "max": 100,
                    "step": 1,
                })
            }
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("è£åˆ‡åå›¾åƒ",)
    FUNCTION = "crop_by_positions"
    CATEGORY = "ğŸŠ Kim-Nodes/ğŸ›‘Icon Processing | å›¾æ ‡å¤„ç†"

    def crop_by_positions(self, å›¾ç‰‡, ä½ç½®æ•°æ®, èµ·å§‹åˆ—å·, ç»ˆæ­¢åˆ—å·, è¡Œå·):
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print("\n=== è¾“å…¥å‚æ•° ===")
        print(f"èµ·å§‹åˆ—å·: {èµ·å§‹åˆ—å·}")
        print(f"ç»ˆæ­¢åˆ—å·: {ç»ˆæ­¢åˆ—å·}")
        print(f"ç»“æŸè¡Œå·: {è¡Œå·}")
        
        # å¤„ç†è¾“å…¥å›¾ç‰‡
        if isinstance(å›¾ç‰‡, torch.Tensor):
            if å›¾ç‰‡.shape[0] != 1:
                raise ValueError("å›¾ç‰‡åªæ”¯æŒ batch_size=1")
            if å›¾ç‰‡.shape[1] in (3, 4):
                å›¾ç‰‡ = å›¾ç‰‡.permute(0, 2, 3, 1)
            image_np = (å›¾ç‰‡[0].cpu().numpy() * 255).astype(np.uint8)
            image_pil = Image.fromarray(image_np)
        else:
            raise ValueError("å›¾ç‰‡å¿…é¡»æ˜¯ torch.Tensor ç±»å‹")

        # æ‰¾åˆ°å››ä¸ªè§’è½çš„ä¸­å¿ƒç‚¹
        å·¦ä¸Šä¸­å¿ƒç‚¹ = None
        å³ä¸Šä¸­å¿ƒç‚¹ = None
        å·¦ä¸‹ä¸­å¿ƒç‚¹ = None
        å³ä¸‹ä¸­å¿ƒç‚¹ = None
        
        # éå†æ‰€æœ‰ä½ç½®æ‰¾åˆ°å››ä¸ªè§’è½
        for pos in ä½ç½®æ•°æ®:
            if pos["åˆ—å·"] == èµ·å§‹åˆ—å· and pos["é‡å¤ç»„å·"] == 0:
                center_x = pos["x"] + pos["å®½"]/2
                center_y = pos["y"] + pos["é«˜"]/2
                å·¦ä¸Šä¸­å¿ƒç‚¹ = (center_x, center_y)
            
            if pos["åˆ—å·"] == ç»ˆæ­¢åˆ—å· and pos["é‡å¤ç»„å·"] == 0:
                center_x = pos["x"] + pos["å®½"]/2
                center_y = pos["y"] + pos["é«˜"]/2
                å³ä¸Šä¸­å¿ƒç‚¹ = (center_x, center_y)
                
            if pos["åˆ—å·"] == èµ·å§‹åˆ—å· and pos["é‡å¤ç»„å·"] == è¡Œå·:
                center_x = pos["x"] + pos["å®½"]/2
                center_y = pos["y"] + pos["é«˜"]/2
                å·¦ä¸‹ä¸­å¿ƒç‚¹ = (center_x, center_y)
                
            if pos["åˆ—å·"] == ç»ˆæ­¢åˆ—å· and pos["é‡å¤ç»„å·"] == è¡Œå·:
                center_x = pos["x"] + pos["å®½"]/2
                center_y = pos["y"] + pos["é«˜"]/2
                å³ä¸‹ä¸­å¿ƒç‚¹ = (center_x, center_y)
        
        if not all([å·¦ä¸Šä¸­å¿ƒç‚¹, å³ä¸Šä¸­å¿ƒç‚¹, å·¦ä¸‹ä¸­å¿ƒç‚¹, å³ä¸‹ä¸­å¿ƒç‚¹]):
            raise ValueError(f"æœªæ‰¾åˆ°æ‰€éœ€çš„å››ä¸ªè§’è½ç‚¹ï¼Œè¯·æ£€æŸ¥å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚\n"
                          f"éœ€è¦çš„èŒƒå›´ï¼šç¬¬{èµ·å§‹åˆ—å·}åˆ—åˆ°ç¬¬{ç»ˆæ­¢åˆ—å·}åˆ—ï¼Œç¬¬0è¡Œåˆ°ç¬¬{è¡Œå·}è¡Œ")

        print(f"\n=== å››ä¸ªè§’è½çš„ä¸­å¿ƒç‚¹ä½ç½® ===")
        print(f"å·¦ä¸Šä¸­å¿ƒç‚¹: {å·¦ä¸Šä¸­å¿ƒç‚¹}")
        print(f"å³ä¸Šä¸­å¿ƒç‚¹: {å³ä¸Šä¸­å¿ƒç‚¹}")
        print(f"å·¦ä¸‹ä¸­å¿ƒç‚¹: {å·¦ä¸‹ä¸­å¿ƒç‚¹}")
        print(f"å³ä¸‹ä¸­å¿ƒç‚¹: {å³ä¸‹ä¸­å¿ƒç‚¹}")

        # è®¡ç®—è£åˆ‡åŒºåŸŸ
        left = int(min(å·¦ä¸Šä¸­å¿ƒç‚¹[0], å·¦ä¸‹ä¸­å¿ƒç‚¹[0]))
        right = int(max(å³ä¸Šä¸­å¿ƒç‚¹[0], å³ä¸‹ä¸­å¿ƒç‚¹[0]))
        top = int(min(å·¦ä¸Šä¸­å¿ƒç‚¹[1], å³ä¸Šä¸­å¿ƒç‚¹[1]))
        bottom = int(max(å·¦ä¸‹ä¸­å¿ƒç‚¹[1], å³ä¸‹ä¸­å¿ƒç‚¹[1]))

        print(f"\n=== æœ€ç»ˆè£åˆ‡åŒºåŸŸ ===")
        print(f"å·¦ä¸Šè§’: ({left}, {top})")
        print(f"å³ä¸‹è§’: ({right}, {bottom})")
        print(f"å®½åº¦: {right - left}, é«˜åº¦: {bottom - top}")

        # è£åˆ‡å›¾ç‰‡
        cropped_image = image_pil.crop((left, top, right, bottom))

        # è½¬æ¢å› tensor
        result = np.array(cropped_image, dtype=np.float32) / 255.0
        if result.shape[-1] == 4:
            result = result[..., :3]  # å»æ‰ alpha é€šé“
        result = np.expand_dims(result, axis=0)
        
        return (torch.from_numpy(result),) 