from PIL import Image 
import numpy as np
import logging
import os

# å¯¼å…¥ torch
try:
    import torch
except ImportError:
    torch = None  # å¦‚æœæœªå®‰è£… torchï¼Œåˆ™å¤„ç†ä¸º None

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Crop_Paste:
    """
    Node for merging a single cropped image back into the original image
    based on the bounding box coordinates.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),  # è¾“å…¥çš„åŸå§‹å›¾ç‰‡
                "crop_image": ("IMAGE",),    # ç½®ä¿¡åº¦æœ€é«˜çš„è£å‰ªå›¾åƒ
                "data": ("DATA",),              # åŒ…å«è¾¹ç•Œæ¡†ä¿¡æ¯çš„å­—å…¸
            },
        }

    RETURN_TYPES = ("IMAGE",)  # è¿”å›ä¿®æ”¹åçš„å›¾ç‰‡
    FUNCTION = "crop_paste"
    CATEGORY = "ğŸŠ Kim-Nodes/âœ‚ Crop | è£å‰ªå¤„ç†"

    def __init__(self):
        pass

    def crop_paste(self, data, image, crop_image):
        """
        Merge the single cropped image back into the original image.
        """
        # æ‰“å°è¾“å…¥å›¾åƒçš„ç±»å‹å’Œç»´åº¦ä¿¡æ¯
        logger.info(f"Original input type: {type(image)}")
        if isinstance(image, torch.Tensor):
            logger.info(f"Original input shape (torch.Tensor): {image.shape}")
        elif isinstance(image, np.ndarray):
            logger.info(f"Original input shape (NumPy array): {image.shape}")
        elif isinstance(image, Image.Image):
            logger.info(f"Original input size (PIL.Image): {image.size}")
        else:
            logger.warning(f"Unknown image type: {type(image)}")

        # ç¡®ä¿è¾“å…¥å›¾åƒæ˜¯ PIL.Image
        image = self._ensure_pil_image(image)
        crop_image = self._ensure_pil_image(crop_image)

        # å†æ¬¡æ‰“å°è½¬æ¢åçš„ PIL.Image çš„å°ºå¯¸
        logger.info(f"Converted input image size (PIL.Image): {image.size}")

        # ç¡®ä¿åŸå§‹å›¾åƒå’Œè£å‰ªå›¾åƒéƒ½æ˜¯ 'RGB' æ¨¡å¼
        if image.mode != 'RGB':
            logger.info(f"Converting original image from {image.mode} to RGB")
            image = image.convert('RGB')

        if crop_image.mode != 'RGB':
            logger.info(f"Converting cropped image from {crop_image.mode} to RGB")
            crop_image = crop_image.convert('RGB')

        # æå–å›¾åƒå°ºå¯¸å’Œè¾¹ç•Œæ¡†
        width, height = image.size  # width å’Œ height å¯¹åº”äº W å’Œ H
        bboxes = data.get("bboxes", [])
        logger.info(f"Original image dimensions: {width}x{height}")
        logger.info(f"Number of bounding boxes: {len(bboxes)}")

        if not bboxes:
            logger.warning("No bounding boxes detected. Returning the original image.")
            return self.process_output(image)  # è¿”å›åŸå§‹å›¾ç‰‡

        # å‡è®¾ç½®ä¿¡åº¦æœ€é«˜çš„è¾¹ç•Œæ¡†æ˜¯ç¬¬ä¸€ä¸ª
        bbox = bboxes[0]
        logger.info(f"Selected bounding box: {bbox}")

        # è®¡ç®—å®é™…è£åˆ‡çš„åƒç´ åæ ‡
        left = max(0, int(bbox["xmin"] * width))
        top = max(0, int(bbox["ymin"] * height))
        right = min(width, int(bbox["xmax"] * width))
        bottom = min(height, int(bbox["ymax"] * height))

        logger.info(f"Bounding box coordinates: left={left}, top={top}, right={right}, bottom={bottom}")

        if left >= right or top >= bottom:
            logger.warning(f"Invalid bounding box: {left}, {top}, {right}, {bottom}")
            return self.process_output(image)

        # è°ƒæ•´è£å‰ªå›¾åƒçš„å°ºå¯¸ä»¥åŒ¹é…è¾¹ç•Œæ¡†çš„å°ºå¯¸
        bbox_width = right - left
        bbox_height = bottom - top
        logger.info(f"BBox width: {bbox_width}, BBox height: {bbox_height}")
        crop_image_resized = crop_image.resize((bbox_width, bbox_height))

        # ç¡®ä¿è£å‰ªçš„å›¾åƒä¸åŸå§‹å›¾åƒæ¨¡å¼ä¸€è‡´
        if crop_image_resized.mode != image.mode:
            logger.info(f"Converting resized cropped image from {crop_image_resized.mode} to {image.mode}")
            crop_image_resized = crop_image_resized.convert(image.mode)

        # å°†è£å‰ªå›¾åƒç²˜è´´å›åŸå§‹å›¾åƒ
        image_paste = image.copy()
        image_paste.paste(crop_image_resized, (left, top))

        logger.info(f"Result image size: {image_paste.size}")
        return self.process_output(image_paste)

    def _ensure_pil_image(self, image):
        """
        Ensure that the input is a PIL.Image. Convert if necessary.
        """
        if isinstance(image, Image.Image):
            return image
        elif isinstance(image, np.ndarray):
            return self._convert_to_image(image)
        elif torch and isinstance(image, torch.Tensor):
            logger.info(f"Input image tensor shape before squeeze: {image.shape}")
            image = image.squeeze()  # å»æ‰å¤šä½™çš„ç»´åº¦
            logger.info(f"Image shape after squeeze (torch.Tensor): {image.shape}")

            if image.ndim == 3:
                if image.shape[0] in [1, 3, 4]:  # (C, H, W)
                    image_np = image.permute(1, 2, 0).cpu().numpy()
                elif image.shape[2] in [1, 3, 4]:  # (H, W, C)
                    image_np = image.cpu().numpy()
                else:
                    raise ValueError(f"Unsupported tensor shape: {image.shape}")
            else:
                raise ValueError(f"Unsupported tensor shape: {image.shape}")

            return self._convert_to_image(image_np)
        else:
            raise ValueError("è¾“å…¥çš„å›¾ç‰‡å¿…é¡»æ˜¯ PIL.Imageã€NumPy æˆ– torch.Tensor ç±»å‹ã€‚")

    def _convert_to_image(self, array):
        """
        Convert a NumPy array to a PIL.Image.
        """
        # å¦‚æœæ•°ç»„æ˜¯å¤šç»´ï¼Œå°è¯•å»æ‰æ— ç”¨çš„ç»´åº¦
        if array.ndim > 3:
            array = array.squeeze()
        logger.info(f"Image shape after squeeze in _convert_to_image: {array.shape}")

        # æ£€æŸ¥æ•°æ®èŒƒå›´å¹¶è½¬æ¢ä¸º [0, 255]
        if array.max() <= 1.0:
            array = (array * 255).astype(np.uint8)
            logger.info("Normalized array to uint8 with range [0, 255]")
        elif array.dtype != np.uint8:
            array = array.astype(np.uint8)

        # åˆ›å»º PIL.Image
        if array.ndim == 2:  # å•é€šé“ç°åº¦å›¾
            image = Image.fromarray(array, mode='L')
        elif array.ndim == 3:
            if array.shape[-1] == 1:
                array = array.squeeze(-1)
                image = Image.fromarray(array, mode='L')
            elif array.shape[-1] == 3:
                image = Image.fromarray(array, mode='RGB')
            elif array.shape[-1] == 4:
                array = array[..., :3]
                image = Image.fromarray(array, mode='RGB')
            else:
                raise ValueError(f"Unsupported number of channels: {array.shape[-1]}")
        else:
            raise ValueError(f"æ— æ³•å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºå›¾åƒï¼Œå½¢çŠ¶: {array.shape}")

        return image

    def process_output(self, image):
        """
        Process the final image to match the output format.
        Converts to a torch.Tensor of shape (1, H, W, 3).
        """
        # å°† PIL.Image è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (H, W, C)
        result_image = np.array(image).astype(np.float32) / 255.0
        logger.info(f"Result image shape: {result_image.shape}")

        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œå½¢çŠ¶ä¸º (1, H, W, C)
        result_image = np.expand_dims(result_image, axis=0)
        logger.info(f"Final image shape with batch dimension: {result_image.shape}")

        # è½¬æ¢ä¸º torch.Tensorï¼Œå½¢çŠ¶ä¸º (1, H, W, C)
        result_tensor = torch.from_numpy(result_image)
        logger.info(f"Final PyTorch tensor shape: {result_tensor.shape}")

        # è¿”å›å¼ é‡ï¼Œå½¢çŠ¶ä¸º (1, H, W, 3)
        return (result_tensor,)
