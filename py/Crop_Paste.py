from PIL import Image 
import numpy as np
import logging
import os

# å¯¼å…¥ torch
try:
    import torch
except ImportError:
    torch = None  # å¦‚æœæœªå®‰è£… torchï¼Œåˆ™å¤„ç†ä¸º None

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Crop_Paste:
    """
    Node for merging a single cropped image back into the original image
    based on the bounding box coordinates.
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),  # è¾“å…¥çš„åŸå§‹å›¾ç‰‡
                "crop_images": ("IMAGE",),    # è£å‰ªçš„å›¾åƒåˆ—è¡¨
                "data": ("DATA",),              # åŒ…å«è¾¹ç•Œæ¡†ä¿¡æ¯çš„å­—å…¸
                "feather_amount": ("FLOAT", {
                    "default": 0.2,
                    "min": 0.0,
                    "max": 0.5,
                    "step": 0.01
                }),  # è¾¹ç¼˜æ¸å˜ç¨‹åº¦æ§åˆ¶
            },
        }

    RETURN_TYPES = ("IMAGE",)  # è¿”å›ä¿®æ”¹åçš„å›¾ç‰‡
    FUNCTION = "crop_paste"
    CATEGORY = "ğŸŠ Kim-Nodes/âœ‚ Crop | è£å‰ªå¤„ç†"

    def __init__(self):
        pass

    def crop_paste(self, data, image, crop_images, feather_amount):
        """
        å°†å¤šä¸ªè£å‰ªçš„å›¾åƒç²˜è´´å›åŸå§‹å›¾åƒçš„å¯¹åº”ä½ç½®
        """
        print("\n===== Crop_Paste èŠ‚ç‚¹è¾“å…¥ä¿¡æ¯ =====")
        print(f"è¾“å…¥å›¾åƒç±»å‹: {type(image)}")
        
        # æ‰“å°è£å‰ªå›¾åƒä¿¡æ¯
        print(f"è£å‰ªå›¾åƒç±»å‹: {type(crop_images)}")
        if isinstance(crop_images, torch.Tensor):
            print(f"è£å‰ªå›¾åƒå½¢çŠ¶: {crop_images.shape}")
            if len(crop_images.shape) == 4:
                print(f"è£å‰ªå›¾åƒæ‰¹æ¬¡æ•°é‡: {crop_images.shape[0]}")
        
        # å¤„ç†åŸå§‹å›¾åƒ
        if isinstance(image, torch.Tensor):
            if len(image.shape) == 4:
                image = image[0]
            image_np = image.cpu().numpy()
            if image_np.max() <= 1.0:
                image_np = (image_np * 255).astype(np.uint8)
            image_pil = Image.fromarray(image_np)
            print(f"åŸå§‹å›¾åƒå°ºå¯¸: {image_pil.size}")
        
        # è·å–è¾¹ç•Œæ¡†
        bboxes = data.get("bboxes", [])
        print(f"è¾¹ç•Œæ¡†æ•°é‡: {len(bboxes)}")
        
        # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾åƒç”¨äºç²˜è´´
        image_paste = image_pil.copy()
        width, height = image_pil.size
        
        if image_paste.mode != 'RGB':
            image_paste = image_paste.convert('RGB')
        
        # å¤„ç†è£å‰ªå›¾åƒ
        print("\nå¤„ç†è£å‰ªå›¾åƒ...")
        if isinstance(crop_images, torch.Tensor) and len(crop_images.shape) == 4:
            batch_size = crop_images.shape[0]
            print(f"æ£€æµ‹åˆ°æ‰¹æ¬¡å¼ é‡ï¼ŒåŒ…å« {batch_size} ä¸ªå›¾åƒ")
            
            # éå†æ‰€æœ‰è£å‰ªå›¾åƒå’Œå¯¹åº”çš„è¾¹ç•Œæ¡†
            for i in range(min(batch_size, len(bboxes))):
                print(f"\nå¤„ç†ç¬¬ {i+1}/{batch_size} ä¸ªè£å‰ªå›¾åƒ")
                
                try:
                    # å®‰å…¨è·å–å›¾åƒ
                    crop_img = crop_images[i]
                    if crop_img.shape[0] == 1 and crop_img.shape[1] == 1:
                        print(f"è­¦å‘Š: å›¾åƒ {i} å°ºå¯¸è¿‡å° ({crop_img.shape})ï¼Œè·³è¿‡")
                        continue
                    
                    # æ‰“å°å½¢çŠ¶å’Œæ•°å€¼èŒƒå›´
                    print(f"è£å‰ªå›¾åƒå½¢çŠ¶: {crop_img.shape}")
                    
                    # è£å‰ªå›¾åƒé¢„å¤„ç†
                    crop_np = crop_img.cpu().numpy()
                    
                    # è§„èŒƒåŒ–æ•°å€¼èŒƒå›´
                    if crop_np.max() > 1.0 or crop_np.min() < 0:
                        print(f"è­¦å‘Š: å›¾åƒ {i} æ•°å€¼èŒƒå›´å¼‚å¸¸ ({crop_np.min()} åˆ° {crop_np.max()})ï¼Œè¿›è¡Œè§„èŒƒåŒ–")
                        crop_np = np.clip(crop_np, 0, 1)
                    
                    # è½¬æ¢ä¸º8ä½æ ¼å¼
                    crop_np = (crop_np * 255).astype(np.uint8)
                    
                    # è®¡ç®—è¾¹ç•Œæ¡†åæ ‡
                    bbox = bboxes[i]
                    left = max(0, int(bbox["xmin"] * width))
                    top = max(0, int(bbox["ymin"] * height))
                    right = min(width, int(bbox["xmax"] * width))
                    bottom = min(height, int(bbox["ymax"] * height))
                    
                    print(f"è¾¹ç•Œæ¡†åæ ‡: ({left}, {top}) -> ({right}, {bottom})")
                    
                    if left >= right or top >= bottom:
                        print(f"è­¦å‘Š: æ— æ•ˆçš„è¾¹ç•Œæ¡†åæ ‡ï¼Œè·³è¿‡")
                        continue
                    
                    # åˆ›å»ºPILå›¾åƒå¹¶è°ƒæ•´å°ºå¯¸
                    crop_pil = Image.fromarray(crop_np)
                    bbox_width = right - left
                    bbox_height = bottom - top
                    crop_resized = crop_pil.resize((bbox_width, bbox_height))
                    
                    # ç²˜è´´å›¾åƒ
                    image_paste.paste(crop_resized, (left, top))
                    print(f"å›¾åƒ {i} ç²˜è´´æˆåŠŸ")
                    
                except Exception as e:
                    print(f"å¤„ç†å›¾åƒ {i} æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
        
        print("\næ‰€æœ‰è£å‰ªå›¾åƒå¤„ç†å®Œæˆ")
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶è¿”å›
        result_np = np.array(image_paste).astype(np.float32) / 255.0
        result_tensor = torch.from_numpy(result_np).unsqueeze(0)
        
        return (result_tensor,)

    def _ensure_pil_image(self, image):
        """
        Ensure that the input is a PIL.Image. Convert if necessary.
        """
        if isinstance(image, Image.Image):
            return image
        elif isinstance(image, np.ndarray):
            return self._convert_to_image(image)
        elif torch and isinstance(image, torch.Tensor):
            logger.info(f"Input image tensor shape before squeeze: {image.shape}")
            image = image.squeeze()  # å»æ‰å¤šä½™çš„ç»´åº¦
            logger.info(f"Image shape after squeeze (torch.Tensor): {image.shape}")

            if image.ndim == 3:
                if image.shape[0] in [1, 3, 4]:  # (C, H, W)
                    image_np = image.permute(1, 2, 0).cpu().numpy()
                elif image.shape[2] in [1, 3, 4]:  # (H, W, C)
                    image_np = image.cpu().numpy()
                else:
                    raise ValueError(f"Unsupported tensor shape: {image.shape}")
            else:
                raise ValueError(f"Unsupported tensor shape: {image.shape}")

            return self._convert_to_image(image_np)
        else:
            raise ValueError("è¾“å…¥çš„å›¾ç‰‡å¿…é¡»æ˜¯ PIL.Imageã€NumPy æˆ– torch.Tensor ç±»å‹ã€‚")

    def _convert_to_image(self, array):
        """
        Convert a NumPy array to a PIL.Image.
        """
        # å¦‚æœæ•°ç»„æ˜¯å¤šç»´ï¼Œå°è¯•å»æ‰æ— ç”¨çš„ç»´åº¦
        if array.ndim > 3:
            array = array.squeeze()
        logger.info(f"Image shape after squeeze in _convert_to_image: {array.shape}")

        # æ£€æŸ¥æ•°æ®èŒƒå›´å¹¶è½¬æ¢ä¸º [0, 255]
        if array.max() <= 1.0:
            array = (array * 255).astype(np.uint8)
            logger.info("Normalized array to uint8 with range [0, 255]")
        elif array.dtype != np.uint8:
            array = array.astype(np.uint8)

        # åˆ›å»º PIL.Image
        if array.ndim == 2:  # å•é€šé“ç°åº¦å›¾
            image = Image.fromarray(array, mode='L')
        elif array.ndim == 3:
            if array.shape[-1] == 1:
                array = array.squeeze(-1)
                image = Image.fromarray(array, mode='L')
            elif array.shape[-1] == 3:
                image = Image.fromarray(array, mode='RGB')
            elif array.shape[-1] == 4:
                array = array[..., :3]
                image = Image.fromarray(array, mode='RGB')
            else:
                raise ValueError(f"Unsupported number of channels: {array.shape[-1]}")
        else:
            raise ValueError(f"æ— æ³•å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸ºå›¾åƒï¼Œå½¢çŠ¶: {array.shape}")

        return image

    def process_output(self, image):
        """
        Process the final image to match the output format.
        Converts to a torch.Tensor of shape (1, H, W, 3).
        """
        # å°† PIL.Image è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (H, W, C)
        result_image = np.array(image).astype(np.float32) / 255.0
        logger.info(f"Result image shape: {result_image.shape}")

        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼Œå½¢çŠ¶ä¸º (1, H, W, C)
        result_image = np.expand_dims(result_image, axis=0)
        logger.info(f"Final image shape with batch dimension: {result_image.shape}")

        # è½¬æ¢ä¸º torch.Tensorï¼Œå½¢çŠ¶ä¸º (1, H, W, C)
        result_tensor = torch.from_numpy(result_image)
        logger.info(f"Final PyTorch tensor shape: {result_tensor.shape}")

        # è¿”å›å¼ é‡ï¼Œå½¢çŠ¶ä¸º (1, H, W, 3)
        return (result_tensor,)
