from PIL import Image, ImageOps
import numpy as np
from ultralytics import YOLO
import torch
import logging
import os

# å°†æ—¥å¿—çº§åˆ«è®¾ç½®ä¸ºERRORï¼Œåªä¿ç•™é”™è¯¯ä¿¡æ¯
logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)

# è®¾ç½®æ¨¡å‹ç›®å½•ä¸ºå½“å‰è·¯å¾„ä¸‹çš„ /models/yolo
model_path = os.path.abspath(os.path.join(os.getcwd(), "models", "yolo"))


# å®šä¹‰ get_files å‡½æ•°
def get_files(directory, extensions):
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹æŒ‡å®šæ‰©å±•åçš„æ–‡ä»¶ã€‚

    :param directory: è¦æœç´¢çš„ç›®å½•è·¯å¾„
    :param extensions: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼Œä¾‹å¦‚ [".pt"]
    :return: å­—å…¸ï¼Œé”®ä¸ºæ–‡ä»¶åï¼Œå€¼ä¸ºå®Œæ•´è·¯å¾„
    """
    files = {}
    if not os.path.exists(directory):  # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        logging.warning(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
        return files

    for filename in os.listdir(directory):
        if any(filename.endswith(ext) for ext in extensions):  # åŒ¹é…æ‰©å±•å
            files[filename] = os.path.join(directory, filename)
    return files


class YOLO_Multi_Crop:
    """
    å¤šäººç‰©æ£€æµ‹å’Œè£å‰ªèŠ‚ç‚¹ï¼Œå¯ä»¥æ£€æµ‹å›¾åƒä¸­çš„å¤šä¸ªäººç‰©å¹¶åˆ†åˆ«è£å‰ªã€‚
    """
    model_cache = {}  # æ¨¡å‹ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½

    @classmethod
    def INPUT_TYPES(cls):
        model_ext = [".pt"]
        FILES_DICT = get_files(model_path, model_ext)  # è·å–æ¨¡å‹æ–‡ä»¶åˆ—è¡¨
        FILE_LIST = list(FILES_DICT.keys())  # æå–æ–‡ä»¶ååˆ—è¡¨
        return {
            "required": {
                "image": ("IMAGE",),  # è¾“å…¥å›¾åƒ
                "yolo_model": (FILE_LIST,),  # å¯é€‰æ¨¡å‹æ–‡ä»¶
                "confidence": ("FLOAT", {
                    "default": 0.5, "min": 0.1, "max": 1.0, "step": 0.01
                }),  # ç½®ä¿¡åº¦é˜ˆå€¼
                "square_size": ("FLOAT", {
                    "default": 100.0, "min": 10.0, "max": 200.0, "step": 1.0
                }),  # ä½¿ç”¨å›¾åƒå°ºå¯¸çš„ç™¾åˆ†æ¯”
                "max_detections": ("INT", {
                    "default": 5, "min": 1, "max": 20, "step": 1
                }),  # æœ€å¤§æ£€æµ‹æ•°é‡
            },
        }

    RETURN_TYPES = ("IMAGE", "DATA")
    FUNCTION = "multi_crop"
    OUTPUT_IS_LIST = (True, False)  # å›¾åƒè¾“å‡ºä¸ºåˆ—è¡¨ï¼ŒDATAè¾“å‡ºä¸æ˜¯åˆ—è¡¨
    CATEGORY = "ğŸ’ Kim-Nodes/âœ‚ Crop | è£å‰ªå·¥å…·"

    def __init__(self):
        pass

    def multi_crop(self, image, confidence, square_size, yolo_model, max_detections):
        # æ£€æŸ¥å¹¶åŠ è½½æ¨¡å‹
        if yolo_model in self.model_cache:
            model = self.model_cache[yolo_model]  # ä½¿ç”¨ç¼“å­˜æ¨¡å‹
        else:
            full_model_path = os.path.join(model_path, yolo_model)  # è·å–æ¨¡å‹è·¯å¾„
            try:
                model = YOLO(full_model_path)  # åŠ è½½æ¨¡å‹
                self.model_cache[yolo_model] = model  # ç¼“å­˜æ¨¡å‹
            except Exception as e:
                logger.error(f"åŠ è½½YOLOæ¨¡å‹å¤±è´¥: {e}")
                raise

        device = "cuda" if torch.cuda.is_available() else "cpu"  # ç¡®å®šè®¾å¤‡

        # å¤„ç†è¾“å…¥å›¾åƒ
        if isinstance(image, torch.Tensor):
            if len(image.shape) == 4 and image.shape[-1] in [1, 3, 4]:  # æ£€æŸ¥å½¢çŠ¶
                image = image.permute(0, 3, 1, 2)  # è°ƒæ•´å½¢çŠ¶ä¸º (batch_size, channels, height, width)
            image_np = image.cpu().numpy()[0]  # å»é™¤æ‰¹æ¬¡ç»´åº¦
            image_np = np.transpose(image_np, (1, 2, 0))  # è½¬ä¸º (height, width, channels)
        else:
            raise TypeError("è¾“å…¥çš„ image ä¸æ˜¯ torch.Tensor ç±»å‹")

        # ç¡®ä¿è¾“å…¥ä¸º RGB æ ¼å¼
        channels = image_np.shape[2]
        if channels == 1:
            image_np = np.repeat(image_np, 3, axis=2)  # ç°åº¦å›¾è½¬ RGB
        elif channels == 4:
            image_np = image_np[:, :, :3]  # å»æ‰ alpha é€šé“

        if image_np.dtype != np.uint8:
            image_np = (image_np * 255).astype(np.uint8)  # è½¬æ¢ä¸º 8 ä½åƒç´ å€¼
        image_pil = Image.fromarray(image_np)  # è½¬æ¢ä¸º PIL å›¾åƒ

        # ä½¿ç”¨ YOLO æ¨¡å‹æ£€æµ‹
        results = model.predict(source=image_np, conf=confidence, device=device)

        bboxes = []  # å­˜å‚¨æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†
        width, height = image_np.shape[1], image_np.shape[0]  # è·å–å›¾åƒçš„å®½å’Œé«˜

        for result in results:
            for box in result.boxes:
                xmin, ymin, xmax, ymax = box.xyxy[0].cpu().numpy()
                conf = box.conf.cpu().numpy()
                cls = box.cls.cpu().numpy()

                if cls == 0 and conf >= confidence:  # å‡è®¾ç±»åˆ«0æ˜¯äºº
                    # è®¡ç®—æ£€æµ‹åˆ°çš„äººç‰©æ¡†çš„å®½åº¦å’Œé«˜åº¦
                    person_width = xmax - xmin
                    person_height = ymax - ymin
                    person_size = max(person_width, person_height)  # ä½¿ç”¨è¾ƒå¤§çš„è¾¹ä½œä¸ºåŸºå‡†
                    
                    # è®¡ç®—å›¾åƒçš„æœ€å°è¾¹é•¿
                    min_side = min(width, height)
                    
                    # æ ¹æ®äººç‰©å°ºå¯¸è®¡ç®—åˆé€‚çš„æ–¹å—å¤§å°
                    # æ·»åŠ é¢å¤–çš„è¾¹è·
                    person_margin = 1.5
                    actual_square_size = person_size * person_margin
                    
                    # åº”ç”¨ç”¨æˆ·æŒ‡å®šçš„ç™¾åˆ†æ¯”è°ƒæ•´
                    actual_square_size = actual_square_size * (square_size / 100.0)
                    
                    # è®¡ç®—è¾¹ç•Œæ¡†çš„ä¸­å¿ƒ
                    center_x = (xmin + xmax) / 2
                    center_y = (ymin + ymax) / 2

                    # ä½¿ç”¨å®é™…çš„æ–¹å—å¤§å°
                    half_size = actual_square_size / 2

                    # è®¡ç®—æ–°çš„è¾¹ç•Œæ¡†åæ ‡ï¼Œä¿æŒæ­£æ–¹å½¢å¤§å°
                    xmin_new = max(0, int(center_x - half_size))
                    xmax_new = min(width, int(center_x + half_size))
                    ymin_new = max(0, int(center_y - half_size))
                    ymax_new = min(height, int(center_y + half_size))

                    # å­˜å‚¨å½’ä¸€åŒ–åçš„è¾¹ç•Œæ¡†åæ ‡å’Œåƒç´ åæ ‡
                    bboxes.append({
                        "xmin": xmin_new / width,
                        "ymin": ymin_new / height,
                        "xmax": xmax_new / width,
                        "ymax": ymax_new / height,
                        "xmin_pixel": xmin_new,
                        "ymin_pixel": ymin_new,
                        "xmax_pixel": xmax_new,
                        "ymax_pixel": ymax_new,
                        "confidence": float(conf),
                        "center_x": center_x,  # æ·»åŠ ä¸­å¿ƒç‚¹åæ ‡ç”¨äºæ’åº
                        "center_y": center_y
                    })

        if not bboxes:
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°äººç‰©ï¼Œè¿”å›åŸå›¾å’Œç©ºæ•°æ®
            return ([self.process_single_image(image_pil)], {"pixels": image_np.tolist(), "bboxes": []})

        # æŒ‰ç…§ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹çš„é¡ºåºæ’åºè¾¹ç•Œæ¡†
        # é¦–å…ˆæŒ‰è¡Œåˆ†ç»„ï¼ˆä½¿ç”¨ä¸€ä¸ªé˜ˆå€¼æ¥ç¡®å®šæ˜¯å¦åœ¨åŒä¸€è¡Œï¼‰
        y_threshold = height * 0.1  # 10%çš„å›¾åƒé«˜åº¦ä½œä¸ºé˜ˆå€¼
        
        # æŒ‰yåæ ‡ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼‰ç²—ç•¥æ’åº
        bboxes.sort(key=lambda x: x["center_y"])
        
        # åˆ†ç»„åˆ°ä¸åŒçš„è¡Œ
        rows = []
        current_row = [bboxes[0]]
        
        for i in range(1, len(bboxes)):
            # å¦‚æœå½“å‰è¾¹ç•Œæ¡†ä¸å½“å‰è¡Œçš„ç¬¬ä¸€ä¸ªè¾¹ç•Œæ¡†åœ¨yæ–¹å‘ä¸Šçš„è·ç¦»å°äºé˜ˆå€¼ï¼Œåˆ™è®¤ä¸ºå®ƒä»¬åœ¨åŒä¸€è¡Œ
            if abs(bboxes[i]["center_y"] - current_row[0]["center_y"]) < y_threshold:
                current_row.append(bboxes[i])
            else:
                # å¦åˆ™å¼€å§‹ä¸€ä¸ªæ–°è¡Œ
                rows.append(current_row)
                current_row = [bboxes[i]]
        
        # æ·»åŠ æœ€åä¸€è¡Œ
        if current_row:
            rows.append(current_row)
        
        # å¯¹æ¯ä¸€è¡Œå†…çš„è¾¹ç•Œæ¡†æŒ‰xåæ ‡ï¼ˆä»å·¦åˆ°å³ï¼‰æ’åº
        for row in rows:
            row.sort(key=lambda x: x["center_x"])
        
        # å°†æ’åºåçš„è¾¹ç•Œæ¡†é‡æ–°å±•å¹³ä¸ºä¸€ä¸ªåˆ—è¡¨
        sorted_bboxes = []
        for row in rows:
            sorted_bboxes.extend(row)
        
        # æ›´æ–°è¾¹ç•Œæ¡†åˆ—è¡¨
        bboxes = sorted_bboxes[:max_detections]
        
        # è£å‰ªæ‰€æœ‰æ£€æµ‹åˆ°çš„äººç‰©
        cropped_persons = []
        face_data = []  # ç”¨äºå­˜å‚¨æ¯ä¸ªè„¸éƒ¨çš„è¯¦ç»†ä¿¡æ¯

        for i, bbox in enumerate(bboxes):
            cropped_img = self.crop_person(image_pil, bbox)
            img_width, img_height = cropped_img.size
            
            # è®°å½•æ¯ä¸ªè„¸éƒ¨çš„è¯¦ç»†ä¿¡æ¯
            face_info = {
                "index": i,
                "center_x": float(bbox["center_x"]),
                "center_y": float(bbox["center_y"]),
                "xmin_pixel": int(bbox["xmin_pixel"]),
                "ymin_pixel": int(bbox["ymin_pixel"]),
                "xmax_pixel": int(bbox["xmax_pixel"]),
                "ymax_pixel": int(bbox["ymax_pixel"]),
                "width": img_width,
                "height": img_height,
                "confidence": float(bbox["confidence"])
            }
            face_data.append(face_info)
            
            cropped_persons.append(cropped_img)

        # å¤„ç†æ‰€æœ‰è£å‰ªåçš„å›¾åƒä¸ºå•ç‹¬çš„å¼ é‡åˆ—è¡¨
        processed_images = []
        for i, img in enumerate(cropped_persons):
            processed = self.process_single_image(img)
            processed_images.append(processed)
        
        return (
            processed_images,  # è¿”å›æ‰€æœ‰è£å‰ªåçš„å›¾åƒåˆ—è¡¨
            {
                # ç§»é™¤åƒç´ æ•°æ®ï¼Œé¿å…æ•°æ®è¿‡å¤§
                "bboxes": bboxes,  # è¿”å›æ£€æµ‹åˆ°çš„æ‰€æœ‰è¾¹ç•Œæ¡†
                "face_data": face_data,  # æ·»åŠ ç»“æ„åŒ–çš„è„¸éƒ¨æ•°æ®
                "count": len(bboxes),  # è¿”å›æ£€æµ‹åˆ°çš„äººç‰©æ•°é‡
                "image_size": {"width": width, "height": height}  # æ·»åŠ å›¾åƒå°ºå¯¸ä¿¡æ¯
            }
        )

    def crop_person(self, image, bbox):
        """
        æ ¹æ®è¾¹ç•Œæ¡†è£å‰ªäººç‰©å›¾åƒ
        """
        width, height = image.size  # è·å–å›¾åƒçš„å®½åº¦å’Œé«˜åº¦
        left = bbox["xmin_pixel"]  # ä½¿ç”¨ä¼ å…¥çš„åƒç´ åæ ‡
        top = bbox["ymin_pixel"]
        right = bbox["xmax_pixel"]
        bottom = bbox["ymax_pixel"]

        # ä½¿ç”¨åæ ‡è£å‰ªå›¾åƒ
        cropped_image = image.crop((left, top, right, bottom))
        return cropped_image.convert("RGB") if cropped_image.mode != "RGB" else cropped_image

    def process_single_image(self, pil_image):
        """
        å°†å•ä¸ªPILå›¾åƒè½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼è¿”å›ã€‚
        """
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–
        img_array = np.array(pil_image).astype(np.float32) / 255.0
        
        # ç¡®ä¿æ˜¯RGBæ ¼å¼
        if img_array.shape[-1] == 4:
            img_array = img_array[..., :3]  # å»æ‰alphaé€šé“
        elif len(img_array.shape) == 2 or img_array.shape[-1] == 1:
            # å¤„ç†ç°åº¦å›¾åƒ
            if len(img_array.shape) == 2:
                img_array = np.expand_dims(img_array, axis=-1)
            img_array = np.repeat(img_array, 3, axis=-1)
            
        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦
        img_array = np.expand_dims(img_array, axis=0)
        return torch.from_numpy(img_array)  # è½¬æ¢ä¸ºå¼ é‡ 