import torch
import numpy as np

class Percentage_Cropper:    
    """
    æŒ‰ç…§å›¾ç‰‡å®½é«˜çš„ç™¾åˆ†æ¯”å‘å†…è£åˆ‡å›¾ç‰‡çš„èŠ‚ç‚¹ã€‚
    """
    
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "horizontal_percent": ("FLOAT", {
                    "default": 10.0,
                    "min": 0.0,
                    "max": 100.0,
                    "step": 0.1,
                    "display": "number"
                }),
                "vertical_percent": ("FLOAT", {
                    "default": 10.0,
                    "min": 0.0,
                    "max": 100.0,
                    "step": 0.1,
                    "display": "number"
                })
            }
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "crop_image"
    CATEGORY = "ğŸ’ Kim-Nodes/âœ‚ Crop | è£å‰ªå·¥å…·"

    def crop_image(self, image, horizontal_percent, vertical_percent):
        """
        æŒ‰ç…§ç»™å®šçš„ç™¾åˆ†æ¯”è£åˆ‡å›¾ç‰‡
        
        å‚æ•°:
            image: è¾“å…¥å›¾ç‰‡å¼ é‡ (B,H,W,C)
            horizontal_percent: æ°´å¹³æ–¹å‘è£åˆ‡çš„ç™¾åˆ†æ¯”
            vertical_percent: å‚ç›´æ–¹å‘è£åˆ‡çš„ç™¾åˆ†æ¯”
        """
        
        # ç¡®ä¿è¾“å…¥å›¾ç‰‡æ˜¯æ­£ç¡®çš„æ ¼å¼
        if len(image.shape) != 4:
            image = image.unsqueeze(0)
            
        B, H, W, C = image.shape
        
        # è®¡ç®—éœ€è¦è£åˆ‡çš„åƒç´ æ•°
        h_crop = int(W * (horizontal_percent / 100.0) / 2)
        v_crop = int(H * (vertical_percent / 100.0) / 2)
        
        # è¿›è¡Œè£åˆ‡
        cropped = image[:, v_crop:H-v_crop, h_crop:W-h_crop, :]
        
        return (cropped,) 